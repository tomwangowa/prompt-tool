#!/usr/bin/env python3
"""
å°è©±å¼ UI çµ„ä»¶æ¨¡çµ„
å¯¦ä½œæ‰€æœ‰å°è©±å¼ä»‹é¢çš„ UI æ¸²æŸ“å‡½æ•¸
"""

import streamlit as st
import time
from typing import Dict, Any, List, Optional, Callable

from conversation_types import Message, MessageRole, MessageType, ConversationSession
from conversation_flow import ConversationFlow


def add_chat_css():
    """æ·»åŠ å°è©±å¼ UI çš„è‡ªè¨‚ CSS æ¨£å¼"""
    st.markdown("""
    <style>
    /* è¨Šæ¯å¡ç‰‡æ¨£å¼ */
    .stChatMessage {
        border-radius: 12px;
        margin-bottom: 12px;
        padding: 12px 16px;
    }

    /* åˆ†æçµæœå¡ç‰‡æ¨£å¼ */
    .analysis-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 16px;
        padding: 24px;
        margin: 16px 0;
        color: white;
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
    }

    /* å„ªåŒ–çµæœå¡ç‰‡æ¨£å¼ */
    .optimization-card {
        background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        border-radius: 16px;
        padding: 24px;
        margin: 16px 0;
        box-shadow: 0 4px 12px rgba(79, 172, 254, 0.4);
    }

    /* Metric å¡ç‰‡æ¨£å¼ - ä¿æŒé è¨­é¡è‰²ä»¥ç›¸å®¹ light/dark ä¸»é¡Œ */
    div[data-testid="stMetric"] {
        background: rgba(102, 126, 234, 0.08);
        border-radius: 8px;
        padding: 12px;
        border: 1px solid rgba(102, 126, 234, 0.2);
    }

    div[data-testid="stMetric"] [data-testid="stMetricValue"] {
        font-size: 24px;
        font-weight: 600;
    }

    /* å°è©±å®¹å™¨èª¿æ•´ */
    .main .block-container {
        padding-bottom: 120px;
    }

    /* å›ºå®šåº•éƒ¨è¼¸å…¥æ¡†æ¨£å¼ */
    div[data-testid="stChatInput"] {
        position: sticky;
        bottom: 0;
        background: var(--secondary-background-color);
        padding: 16px 0;
        box-shadow: 0 -4px 12px rgba(0,0,0,0.08);
        z-index: 100;
    }
    </style>
    """, unsafe_allow_html=True)


def render_conversation_ui(t_func: Callable[[str], str], create_llm_func: Callable[[], Any]):
    """
    æ¸²æŸ“å°è©±å¼ UI ä¸»ä»‹é¢

    Args:
        t_func: ç¿»è­¯å‡½æ•¸
        create_llm_func: å‰µå»º LLM å¯¦ä¾‹çš„å‡½æ•¸
    """
    session = st.session_state.current_session

    # æ·»åŠ  CSS æ¨£å¼
    add_chat_css()

    # é¡¯ç¤ºå°è©±æ­·å²
    for msg in session.messages:
        render_message(msg, t_func)

    # æ ¹æ“šç‹€æ…‹æ¸²æŸ“è¼¸å…¥å€åŸŸï¼ˆåŒ…å« token æŒ‡ç¤ºå™¨ï¼‰
    render_input_area(session, t_func, create_llm_func)


def render_token_indicator(session: ConversationSession, t_func: Callable[[str], str], compact: bool = True):
    """
    æ¸²æŸ“ Token ä½¿ç”¨ç‹€æ…‹æŒ‡ç¤ºå™¨

    Args:
        session: å°è©±æœƒè©±
        t_func: ç¿»è­¯å‡½æ•¸
        compact: æ˜¯å¦ä½¿ç”¨ç·Šæ¹Šæ¨¡å¼ï¼ˆé©åˆè¼¸å…¥æ¡†æ—é‚Šï¼‰
    """
    if session.current_context_tokens == 0:
        return  # æ²’æœ‰ token ä½¿ç”¨æ™‚ä¸é¡¯ç¤º

    usage_percentage = session.get_token_usage_percentage()

    # æ ¹æ“šä½¿ç”¨ç‡é¸æ“‡åœ–ç¤º
    if usage_percentage >= 90:
        icon = "ğŸ”´"
    elif usage_percentage >= 70:
        icon = "ğŸŸ¡"
    else:
        icon = "ğŸŸ¢"

    if compact:
        # ç·Šæ¹Šæ¨¡å¼ï¼šå–®è¡Œé¡¯ç¤º + 90% æ™‚çš„å¿«é€Ÿæ“ä½œ
        status_text = f"{icon} {session.current_context_tokens:,} / {session.context_window_limit:,} ({usage_percentage:.1f}%)"

        if usage_percentage >= 90:
            # é«˜å±ç‹€æ…‹ï¼šé¡¯ç¤ºéŒ¯èª¤å’Œä¿å­˜æŒ‰éˆ•
            col1, col2 = st.columns([4, 1])
            with col1:
                st.error(status_text, icon=icon)
            with col2:
                if st.button("ğŸ’¾", key="save_warning_compact", help=t_func("save_now"), type="primary"):
                    st.session_state.show_save_dialog = True
        elif usage_percentage >= 70:
            st.warning(status_text, icon=icon)
        else:
            st.info(status_text, icon=icon)
    else:
        # å®Œæ•´æ¨¡å¼ï¼šé€²åº¦æ¢ + è©³ç´°è³‡è¨Š
        col1, col2 = st.columns([3, 1])

        with col1:
            st.progress(
                min(usage_percentage / 100, 1.0),
                text=f"{icon} {t_func('context_usage')}: {session.current_context_tokens:,} / {session.context_window_limit:,} ({usage_percentage:.1f}%)"
            )

        with col2:
            # ç•¶æ¥è¿‘é™åˆ¶æ™‚é¡¯ç¤ºè­¦å‘ŠæŒ‰éˆ•
            if usage_percentage >= 90:
                if st.button("ğŸ’¾ " + t_func("save_now"), key="save_warning", type="primary"):
                    st.session_state.show_save_dialog = True

        # é¡¯ç¤ºè­¦å‘Šè¨Šæ¯
        if usage_percentage >= 90:
            st.error(t_func("token_limit_warning"))
        elif usage_percentage >= 70:
            st.warning(t_func("token_limit_notice"))


def render_message(msg: Message, t_func: Callable[[str], str]):
    """
    æ¸²æŸ“å–®å€‹è¨Šæ¯

    Args:
        msg: è¨Šæ¯ç‰©ä»¶
        t_func: ç¿»è­¯å‡½æ•¸
    """
    if msg.role == MessageRole.USER:
        # ç”¨æˆ¶è¨Šæ¯
        with st.chat_message("user", avatar="ğŸ§‘"):
            st.write(msg.content)

    elif msg.role == MessageRole.ASSISTANT:
        # AI è¨Šæ¯ - æ ¹æ“šé¡å‹æ¸²æŸ“ä¸åŒçµ„ä»¶
        if msg.type == MessageType.ANALYSIS:
            render_analysis_card(msg, t_func)
        elif msg.type == MessageType.QUESTIONS:
            render_questions_card(msg, t_func)
        elif msg.type == MessageType.OPTIMIZATION:
            render_optimization_card(msg, t_func)
        else:
            # ä¸€èˆ¬æ–‡å­—è¨Šæ¯
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                st.write(msg.content)


def render_analysis_card(msg: Message, t_func: Callable[[str], str]):
    """
    æ¸²æŸ“åˆ†æçµæœå¡ç‰‡

    Args:
        msg: åˆ†æè¨Šæ¯ç‰©ä»¶
        t_func: ç¿»è­¯å‡½æ•¸
    """
    with st.chat_message("assistant", avatar="ğŸ“Š"):
        st.markdown("#### ğŸ“Š " + t_func("analysis_result"))

        if msg.analysis_data:
            analysis = msg.analysis_data

            # è©•åˆ†å±•ç¤ºï¼ˆä½¿ç”¨ 4 æ¬„ï¼‰
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric(
                    t_func("completeness_label"),
                    f"{analysis.get('completeness_score', 0)}/10"
                )
            with col2:
                st.metric(
                    t_func("clarity_label"),
                    f"{analysis.get('clarity_score', 0)}/10"
                )
            with col3:
                st.metric(
                    t_func("structure_label"),
                    f"{analysis.get('structure_score', 0)}/10"
                )
            with col4:
                st.metric(
                    t_func("specificity_label"),
                    f"{analysis.get('specificity_score', 0)}/10"
                )

            # æç¤ºé¡å‹å’Œè¤‡é›œåº¦
            st.info(
                f"**{t_func('prompt_type')}:** {analysis.get('prompt_type', 'unknown')} | "
                f"**{t_func('complexity_level')}:** {analysis.get('complexity_level', 'unknown')}"
            )

            # è©³ç´°åˆ†æï¼ˆå¯å±•é–‹ï¼‰
            with st.expander(t_func("view_details"), expanded=False):
                has_content = False

                if analysis.get('missing_elements'):
                    st.markdown(f"**{t_func('missing_elements')}:**")
                    for elem in analysis['missing_elements']:
                        st.markdown(f"- {elem}")
                    has_content = True

                if analysis.get('improvement_suggestions'):
                    st.markdown(f"**{t_func('improvement_suggestions')}:**")
                    for sugg in analysis['improvement_suggestions']:
                        st.markdown(f"- {sugg}")
                    has_content = True

                # å¦‚æœæ²’æœ‰å…·é«”å»ºè­°ä¸” analysis æœ‰å…§å®¹ï¼Œé¡¯ç¤ºå®Œæ•´åˆ†ææ•¸æ“š
                if not has_content and analysis:
                    st.json(analysis)


def render_questions_card(msg: Message, t_func: Callable[[str], str]):
    """
    æ¸²æŸ“æ”¹é€²å•é¡Œå¡ç‰‡

    Args:
        msg: å•é¡Œè¨Šæ¯ç‰©ä»¶
        t_func: ç¿»è­¯å‡½æ•¸
    """
    with st.chat_message("assistant", avatar="ğŸ’¡"):
        st.markdown("#### ğŸ’¡ " + t_func("improvement_header"))

        if msg.questions_data:
            questions = msg.questions_data

            # æª¢æŸ¥æ­¤è¨Šæ¯æ˜¯å¦ç‚ºæœ€æ–°çš„å¾…å›ç­”å•é¡Œ
            session = st.session_state.current_session
            is_latest_questions = (
                session.pending_questions is not None and
                len(session.messages) > 0 and
                session.messages[-1].id == msg.id or
                (len(session.messages) > 1 and session.messages[-2].id == msg.id)
            )

            # ä½¿ç”¨è¡¨å–®æ”¶é›†æ‰€æœ‰å•é¡Œçš„å›ç­”
            with st.form(key=f"questions_form_{msg.id}"):
                responses = {}
                seen_types = set()  # è¿½è¹¤å·²è¦‹éçš„ typeï¼Œæª¢æ¸¬è¡çª

                for i, q in enumerate(questions):
                    question_text = q.get('question', '')
                    question_type = q.get('type', 'text')

                    # ä½¿ç”¨ question_type ä½œç‚º keyï¼ˆèˆ‡ PromptEvaluator.optimize_prompt çš„æœŸæœ›ä¸€è‡´ï¼‰
                    # è¨­è¨ˆå‡è¨­ï¼šYAML é…ç½®ç¢ºä¿æ¯å€‹å•é¡Œçš„ type å”¯ä¸€ï¼ˆrole, format, detail, scope, reasoningï¼‰
                    response_key = question_type

                    # æª¢æ¸¬ key è¡çªï¼ˆé˜²ç¦¦æ€§ç·¨ç¨‹ï¼‰
                    if response_key in seen_types:
                        # å¦‚æœæª¢æ¸¬åˆ°é‡è¤‡ï¼Œä½¿ç”¨ç´¢å¼•å¾Œç¶´
                        response_key = f"{question_type}_{i}"
                    seen_types.add(question_type)

                    # æ ¹æ“šå•é¡Œé¡å‹æ¸²æŸ“ä¸åŒçš„è¼¸å…¥å…ƒä»¶
                    if question_type == "reasoning" or q.get('input_type') == 'checkbox':
                        # Checkbox é¡å‹
                        responses[response_key] = st.checkbox(
                            question_text,
                            key=f"q_{msg.id}_{i}",
                            disabled=not is_latest_questions
                        )

                    elif q.get('input_type') == 'selectbox' and q.get('options'):
                        # ä¸‹æ‹‰é¸å–®é¡å‹
                        options = q['options']
                        labels = [opt['label'] for opt in options]
                        keys = [opt['key'] for opt in options]

                        selected = st.selectbox(
                            question_text,
                            options=labels,
                            key=f"q_{msg.id}_{i}",
                            disabled=not is_latest_questions
                        )

                        # æ‰¾åˆ°å°æ‡‰çš„ key
                        selected_index = labels.index(selected) if selected in labels else 0
                        responses[response_key] = keys[selected_index]

                    else:
                        # æ–‡å­—è¼¸å…¥é¡å‹
                        responses[response_key] = st.text_input(
                            question_text,
                            key=f"q_{msg.id}_{i}",
                            disabled=not is_latest_questions
                        )

                # æª¢æŸ¥æ˜¯å¦æ­£åœ¨è™•ç†ä¸­
                is_processing = st.session_state.get('is_processing', False)

                # æäº¤æŒ‰éˆ•ï¼ˆåªæœ‰æœ€æ–°çš„å•é¡Œå¯æäº¤ï¼Œä¸”æœªåœ¨è™•ç†ä¸­ï¼‰
                submitted = st.form_submit_button(
                    t_func("processing") if is_processing else t_func("generate_button"),
                    use_container_width=True,
                    disabled=not is_latest_questions or is_processing
                )

                if submitted and is_latest_questions and not is_processing:
                    # ä¿å­˜å›ç­”åˆ° session state
                    st.session_state.pending_responses = responses
                    st.session_state.trigger_optimization = True
                    st.rerun()


def render_optimization_card(msg: Message, t_func: Callable[[str], str]):
    """
    æ¸²æŸ“å„ªåŒ–çµæœå¡ç‰‡

    Args:
        msg: å„ªåŒ–è¨Šæ¯ç‰©ä»¶
        t_func: ç¿»è­¯å‡½æ•¸
    """
    with st.chat_message("assistant", avatar="âœ¨"):
        st.markdown("#### âœ¨ " + t_func("result_header"))

        if msg.optimization_data:
            result = msg.optimization_data

            # ç²å–åŸå§‹å’Œå„ªåŒ–å¾Œçš„æç¤º
            original_prompt = st.session_state.current_session.original_prompt
            enhanced_prompt = result.get("enhanced_prompt", "")

            # å°æ¯”å±•ç¤ºï¼ˆå·¦å³å…©æ¬„ï¼‰
            col1, col2 = st.columns(2)

            with col1:
                st.markdown(f"**ğŸ“„ {t_func('original_prompt')}**")
                st.text_area(
                    label="original",
                    value=original_prompt,
                    height=250,
                    disabled=True,
                    key=f"orig_{msg.id}",
                    label_visibility="collapsed"
                )

            with col2:
                st.markdown(f"**âœ¨ {t_func('enhanced_prompt')}**")
                st.text_area(
                    label="enhanced",
                    value=enhanced_prompt,
                    height=250,
                    key=f"enh_{msg.id}",
                    label_visibility="collapsed"
                )

            # æ”¹é€²èªªæ˜
            if result.get('improvements'):
                with st.expander(t_func("improvement_description"), expanded=True):
                    for improvement in result['improvements']:
                        st.markdown(f"- {improvement}")

            # æç¤ºï¼šå¯ç›´æ¥é¸æ“‡ä¸Šæ–¹æ–‡å­—è¤‡è£½
            st.info("ğŸ’¡ " + t_func("select_to_copy"))

            # æ“ä½œæŒ‰éˆ•
            st.markdown("---")
            col1, col2 = st.columns(2)

            with col1:
                if st.button("ğŸ”„ " + t_func("optimize_again"), key=f"iterate_{msg.id}", use_container_width=True):
                    # è§¸ç™¼æ–°ä¸€è¼ªå„ªåŒ–
                    st.session_state.trigger_iterate = True
                    st.rerun()

            with col2:
                # ä¿å­˜æç¤ºæŒ‰éˆ•
                with st.popover(t_func("save_prompt"), use_container_width=True):
                    render_save_prompt_form(original_prompt, enhanced_prompt, msg.analysis_data, t_func)


def render_save_prompt_form(original_prompt: str, optimized_prompt: str, analysis_scores: Optional[Dict], t_func: Callable[[str], str]):
    """
    æ¸²æŸ“ä¿å­˜æç¤ºè¡¨å–®

    Args:
        original_prompt: åŸå§‹æç¤º
        optimized_prompt: å„ªåŒ–å¾Œçš„æç¤º
        analysis_scores: åˆ†æè©•åˆ†
        t_func: ç¿»è­¯å‡½æ•¸
    """
    save_name = st.text_input(t_func("save_name"))
    save_tags = st.text_input(t_func("save_tags"))

    if st.button(t_func("save_prompt"), key="confirm_save_in_form"):
        if save_name:
            try:
                # è™•ç†æ¨™ç±¤
                tags = [tag.strip() for tag in save_tags.split(",") if tag.strip()] if save_tags else []

                # ä¿å­˜åˆ°è³‡æ–™åº«
                prompt_id = st.session_state.prompt_db.save_prompt(
                    name=save_name,
                    original_prompt=original_prompt,
                    optimized_prompt=optimized_prompt,
                    analysis_scores=analysis_scores,
                    tags=tags,
                    language=st.session_state.language
                )

                # ä½¿å¿«å–å¤±æ•ˆ
                st.session_state.export_cache_key = str(time.time())
                st.success(t_func("save_success"))
                st.rerun()

            except Exception as e:
                st.error(f"{t_func('save_error')}: {str(e)}")
        else:
            st.warning(t_func("please_enter_name"))


def render_input_area(session: ConversationSession, t_func: Callable[[str], str], create_llm_func: Callable[[], Any]):
    """
    æ ¹æ“šæœƒè©±ç‹€æ…‹æ¸²æŸ“è¼¸å…¥å€åŸŸ

    Args:
        session: å°è©±æœƒè©±
        t_func: ç¿»è­¯å‡½æ•¸
        create_llm_func: å‰µå»º LLM å¯¦ä¾‹çš„å‡½æ•¸
    """
    # æª¢æŸ¥æ˜¯å¦æœ‰å¾…è™•ç†çš„æ“ä½œ
    if st.session_state.get('trigger_optimization'):
        # åŸ·è¡Œå„ªåŒ–
        st.session_state.trigger_optimization = False
        st.session_state.is_processing = True  # æ¨™è¨˜è™•ç†ä¸­
        responses = st.session_state.get('pending_responses', {})

        try:
            if responses:
                with st.spinner(t_func("processing")):
                    llm = create_llm_func()
                    flow = ConversationFlow(session, llm, st.session_state.language)
                    result = flow.handle_questions_response(responses)

                    # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
                    optimization_result = result.get("optimization", {})
                    if "error" in optimization_result:
                        st.error(f"Error: {optimization_result.get('error')}")
                        # éŒ¯èª¤æ™‚ä¸ rerunï¼Œè®“éŒ¯èª¤è¨Šæ¯ä¿æŒå¯è¦‹
                    else:
                        st.session_state.current_session = session
                        st.rerun()  # æˆåŠŸæ™‚æ‰ rerun
        except Exception as e:
            # æ•æ‰æœªé æœŸçš„ç•°å¸¸
            st.error(f"An unexpected error occurred: {str(e)}")
        finally:
            st.session_state.is_processing = False  # ç¢ºä¿è™•ç†æ¨™è¨˜è¢«é‡ç½®

    if st.session_state.get('trigger_iterate'):
        # è§¸ç™¼æ–°ä¸€è¼ªå„ªåŒ–
        st.session_state.trigger_iterate = False
        st.session_state.is_processing = True  # æ¨™è¨˜è™•ç†ä¸­

        try:
            with st.spinner(t_func("processing")):
                llm = create_llm_func()
                flow = ConversationFlow(session, llm, st.session_state.language)
                result = flow.handle_initial_prompt(session.current_prompt)

                # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
                analysis_result = result.get("analysis", {})
                if "error" in analysis_result:
                    st.error(f"Error: {analysis_result.get('error')}")
                    # éŒ¯èª¤æ™‚ä¸ rerunï¼Œè®“éŒ¯èª¤è¨Šæ¯ä¿æŒå¯è¦‹
                else:
                    st.session_state.current_session = session
                    st.rerun()  # æˆåŠŸæ™‚æ‰ rerun
        except Exception as e:
            # æ•æ‰æœªé æœŸçš„ç•°å¸¸
            st.error(f"An unexpected error occurred: {str(e)}")
        finally:
            st.session_state.is_processing = False  # ç¢ºä¿è™•ç†æ¨™è¨˜è¢«é‡ç½®

    # åˆ¤æ–·ç•¶å‰éšæ®µ
    has_messages = len(session.messages) > 0
    has_optimization = session.last_optimization is not None
    has_pending_questions = session.pending_questions is not None and len(session.pending_questions) > 0

    # æª¢æŸ¥æ˜¯å¦æ­£åœ¨è™•ç†ä¸­
    is_processing = st.session_state.get('is_processing', False)

    # é¡¯ç¤º Token ä½¿ç”¨ç‹€æ…‹ï¼ˆç·Šæ¹Šæ¨¡å¼ï¼Œåœ¨è¼¸å…¥æ¡†ä¸Šæ–¹ï¼‰
    render_token_indicator(session, t_func, compact=True)

    # è¼¸å…¥å€åŸŸ
    if not has_messages:
        # åˆå§‹ç‹€æ…‹ï¼šé¡¯ç¤ºæç¤ºè¼¸å…¥
        st.markdown("### " + t_func("initial_prompt_header"))

        user_input = st.chat_input(
            placeholder=t_func("chat_input_placeholder"),
            key="initial_chat_input",
            disabled=is_processing
        )

        if user_input:
            # è™•ç†åˆå§‹è¼¸å…¥
            st.session_state.is_processing = True
            try:
                with st.spinner(t_func("processing")):
                    llm = create_llm_func()
                    flow = ConversationFlow(session, llm, st.session_state.language)
                    result = flow.handle_initial_prompt(user_input)

                    # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
                    analysis_result = result.get("analysis", {})
                    if "error" in analysis_result:
                        st.error(f"Error: {analysis_result.get('error')}")
                        # éŒ¯èª¤æ™‚ä¸ rerunï¼Œè®“éŒ¯èª¤è¨Šæ¯ä¿æŒå¯è¦‹
                    else:
                        # æ›´æ–° session
                        st.session_state.current_session = session
                        st.rerun()  # æˆåŠŸæ™‚æ‰ rerun
            except Exception as e:
                # æ•æ‰æœªé æœŸçš„ç•°å¸¸
                st.error(f"An unexpected error occurred: {str(e)}")
            finally:
                st.session_state.is_processing = False

    elif has_optimization:
        # å„ªåŒ–å®Œæˆå¾Œï¼šæ”¯æ´æŒçºŒå°è©±
        user_input = st.chat_input(
            placeholder=t_func("followup_input_placeholder"),
            key="followup_chat_input",
            disabled=is_processing
        )

        if user_input:
            # è™•ç†å¾ŒçºŒå°è©±
            st.session_state.is_processing = True
            try:
                with st.spinner(t_func("processing")):
                    llm = create_llm_func()
                    flow = ConversationFlow(session, llm, st.session_state.language)
                    result = flow.handle_followup_message(user_input)

                    # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
                    if "error" in result:
                        st.error(f"Error: {result.get('error')}")
                        # éŒ¯èª¤æ™‚ä¸ rerunï¼Œè®“éŒ¯èª¤è¨Šæ¯ä¿æŒå¯è¦‹
                    else:
                        # æ›´æ–° session
                        st.session_state.current_session = session
                        st.rerun()  # æˆåŠŸæ™‚æ‰ rerun
            except Exception as e:
                # æ•æ‰æœªé æœŸçš„ç•°å¸¸
                st.error(f"An unexpected error occurred: {str(e)}")
            finally:
                st.session_state.is_processing = False

    elif has_pending_questions:
        # ç­‰å¾…ç”¨æˆ¶å›ç­”å•é¡Œï¼ˆå•é¡Œå·²åœ¨ render_questions_card ä¸­é¡¯ç¤ºï¼‰
        # é€™è£¡åªéœ€æç¤º
        st.info(t_func("please_answer_questions"))

    else:
        # å…¶ä»–ç‹€æ…‹
        st.info(t_func("please_wait"))


def render_new_conversation_button(t_func: Callable[[str], str]):
    """
    æ¸²æŸ“ã€Œé–‹å§‹æ–°å°è©±ã€æŒ‰éˆ•

    Args:
        t_func: ç¿»è­¯å‡½æ•¸
    """
    if st.button("ğŸ”„ " + t_func("new_conversation"), use_container_width=True):
        from conversation_types import create_new_session
        st.session_state.current_session = create_new_session()
        # æ¸…é™¤è§¸ç™¼å™¨
        st.session_state.trigger_optimization = False
        st.session_state.trigger_iterate = False
        st.session_state.pending_responses = {}
        st.rerun()


def get_conversation_ui_translations():
    """
    ç²å–å°è©±å¼ UI æ‰€éœ€çš„é¡å¤–ç¿»è­¯éµ

    Returns:
        ç¿»è­¯å­—å…¸ï¼ˆéœ€è¦åˆä½µåˆ° app.py çš„ translationsï¼‰
    """
    return {
        "zh_TW": {
            "chat_input_placeholder": "è¼¸å…¥æ‚¨è¦å„ªåŒ–çš„æç¤º...",
            "followup_input_placeholder": "æƒ³è¦é€²ä¸€æ­¥èª¿æ•´å—ï¼Ÿè©¦è©¦ã€ŒåŠ ä¸Šç¯„ä¾‹ã€æˆ–ã€Œæ›´æ­£å¼ä¸€é»ã€",
            "new_conversation": "é–‹å§‹æ–°å°è©±",
            "analysis_result": "æç¤ºåˆ†æçµæœ",
            "completeness_label": "å®Œæ•´æ€§",
            "clarity_label": "æ¸…æ™°åº¦",
            "structure_label": "çµæ§‹æ€§",
            "specificity_label": "å…·é«”æ€§",
            "complexity_level": "è¤‡é›œåº¦",
            "view_details": "æŸ¥çœ‹è©³ç´°åˆ†æ",
            "missing_elements": "ç¼ºå¤±å…ƒç´ ",
            "improvement_suggestions": "æ”¹é€²å»ºè­°",
            "please_answer_questions": "è«‹å›ç­”ä¸Šæ–¹çš„æ”¹é€²å•é¡Œ",
            "please_wait": "è«‹ç¨å€™...",
            "please_enter_name": "è«‹è¼¸å…¥æç¤ºåç¨±",
            "select_to_copy": "é¸æ“‡ä¸Šæ–¹æ–‡å­—æ¡†ä¸­çš„å…§å®¹å³å¯è¤‡è£½",
            "context_usage": "ä¸Šä¸‹æ–‡ä½¿ç”¨é‡",
            "save_now": "ç«‹å³ä¿å­˜",
            "token_limit_warning": "âš ï¸ Token ä½¿ç”¨é‡å·²é” 90%ï¼å»ºè­°ç«‹å³ä¿å­˜ç•¶å‰çµæœï¼Œä»¥å…è¶…å‡ºé™åˆ¶ã€‚",
            "token_limit_notice": "ğŸ’¡ Token ä½¿ç”¨é‡å·²é” 70%ï¼Œè«‹æ³¨æ„å°è©±é•·åº¦ã€‚"
        },
        "en": {
            "chat_input_placeholder": "Enter your prompt to optimize...",
            "followup_input_placeholder": "Want to adjust further? Try 'add examples' or 'make it more formal'",
            "new_conversation": "New Conversation",
            "analysis_result": "Prompt Analysis Result",
            "completeness_label": "Completeness",
            "clarity_label": "Clarity",
            "structure_label": "Structure",
            "specificity_label": "Specificity",
            "complexity_level": "Complexity",
            "view_details": "View Details",
            "missing_elements": "Missing Elements",
            "improvement_suggestions": "Improvement Suggestions",
            "please_answer_questions": "Please answer the improvement questions above",
            "please_wait": "Please wait...",
            "please_enter_name": "Please enter a name for the prompt",
            "select_to_copy": "Select text from the text area above to copy",
            "context_usage": "Context Usage",
            "save_now": "Save Now",
            "token_limit_warning": "âš ï¸ Token usage has reached 90%! Please save your results to avoid exceeding the limit.",
            "token_limit_notice": "ğŸ’¡ Token usage has reached 70%. Please monitor conversation length."
        },
        "ja": {
            "chat_input_placeholder": "æœ€é©åŒ–ã—ãŸã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
            "followup_input_placeholder": "ã•ã‚‰ã«èª¿æ•´ã—ã¾ã™ã‹ï¼Ÿã€Œä¾‹ã‚’è¿½åŠ ã€ã¾ãŸã¯ã€Œã‚ˆã‚Šãƒ•ã‚©ãƒ¼ãƒãƒ«ã«ã€ãªã©è©¦ã—ã¦ãã ã•ã„",
            "new_conversation": "æ–°ã—ã„ä¼šè©±",
            "analysis_result": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆ†æçµæœ",
            "completeness_label": "å®Œå…¨æ€§",
            "clarity_label": "æ˜ç¢ºæ€§",
            "structure_label": "æ§‹é€ æ€§",
            "specificity_label": "å…·ä½“æ€§",
            "complexity_level": "è¤‡é›‘åº¦",
            "view_details": "è©³ç´°ã‚’è¡¨ç¤º",
            "missing_elements": "æ¬ è½è¦ç´ ",
            "improvement_suggestions": "æ”¹å–„ææ¡ˆ",
            "please_answer_questions": "ä¸Šè¨˜ã®æ”¹å–„è³ªå•ã«ç­”ãˆã¦ãã ã•ã„",
            "please_wait": "ãŠå¾…ã¡ãã ã•ã„...",
            "please_enter_name": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            "select_to_copy": "ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠã—ã¦ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„",
            "context_usage": "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½¿ç”¨é‡",
            "save_now": "ä»Šã™ãä¿å­˜",
            "token_limit_warning": "âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ãŒ90%ã«é”ã—ã¾ã—ãŸï¼åˆ¶é™ã‚’è¶…ãˆãªã„ã‚ˆã†ã«çµæœã‚’ä¿å­˜ã—ã¦ãã ã•ã„ã€‚",
            "token_limit_notice": "ğŸ’¡ ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ãŒ70%ã«é”ã—ã¾ã—ãŸã€‚ä¼šè©±ã®é•·ã•ã«ã”æ³¨æ„ãã ã•ã„ã€‚"
        }
    }
