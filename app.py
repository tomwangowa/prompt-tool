import streamlit as st
import os
import time
from datetime import datetime
from llm_invoker import LLMFactory, ParameterPresets
from prompt_eval import PromptEvaluator
from prompt_database import PromptDatabase
from prompt_storage_local import LocalStoragePromptDB
from config_loader import get_default_config_loader
from conversation_types import create_new_session, ConversationSession, Message, MessageRole, MessageType
from conversation_ui import render_conversation_ui, render_new_conversation_button, get_conversation_ui_translations

max_token_length = 131072  # Claude çš„æœ€å¤§ tokens é™åˆ¶


# ç¿»è­¯å­—å…¸
translations = {
    "zh_TW": {  # ç¹é«”ä¸­æ–‡
        "app_title": "AI æç¤ºå·¥ç¨‹é¡§å•",
        "initial_prompt_header": "è¼¸å…¥æ‚¨çš„åˆå§‹æç¤º",
        "initial_prompt_label": "åœ¨æ­¤è¼¸å…¥æ‚¨æƒ³è¦å„ªåŒ–çš„æç¤º",
        "analyze_button": "åˆ†ææç¤º",
        "please_input": "è«‹å…ˆè¼¸å…¥æç¤º",
        "improvement_header": "è®“æˆ‘å€‘æ”¹é€²æ‚¨çš„æç¤º",
        "generate_button": "ç”Ÿæˆå„ªåŒ–æç¤º",
        "result_header": "å„ªåŒ–æç¤ºçµæœ",
        "original_prompt": "åŸå§‹æç¤º",
        "enhanced_prompt": "å„ªåŒ–å¾Œçš„æç¤º",
        "copy_text": "å¯ä»¥è¤‡è£½ä½¿ç”¨",
        "improvement_description": "æ”¹é€²èªªæ˜",
        "optimize_again": "å†æ¬¡å„ªåŒ–",
        "restart": "é‡æ–°é–‹å§‹",
        "test_connection": "æ¸¬è©¦é€£æ¥",
        "connection_success": "é€£æ¥æ­£å¸¸",
        "connection_error": "é€£æ¥éŒ¯èª¤",
        "aws_settings": "LLM è¨­ç½®",
        "select_llm": "é¸æ“‡ LLM æ¨¡å‹",
        "select_preset": "é¸æ“‡åƒæ•¸é è¨­",
        "select_region": "é¸æ“‡ AWS å€åŸŸ",
        "model_params": "æ¨¡å‹åƒæ•¸",
        "max_tokens": "æœ€å¤§è¼¸å‡ºä»¤ç‰Œæ•¸",
        "system_prompt": "ç³»çµ±æç¤º",
        "user_prompt": "ç”¨æˆ¶æç¤º",
        "estimated_tokens": "ä¼°è¨ˆè¼¸å…¥ä»¤ç‰Œæ•¸é‡",
        "execute_button": "åŸ·è¡Œ",
        "processing": "æ­£åœ¨è™•ç†...",
        "output_result": "è¼¸å‡ºçµæœ",
        "usage_info": "ä½¿ç”¨æƒ…æ³",
        "input_tokens": "è¼¸å…¥ä»¤ç‰Œ",
        "output_tokens": "è¼¸å‡ºä»¤ç‰Œ",
        "total_tokens": "ç¸½ä»¤ç‰Œ",
        "processing_time": "è™•ç†æ™‚é–“",
        "api_error": "èª¿ç”¨ API æ™‚ç™¼ç”ŸéŒ¯èª¤",
        "custom_preset": "è‡ªå®šç¾©",
        "prompt_type": "æç¤ºé¡å‹",
        "prompt_types": {
            "zero_shot": "é›¶æ¨£æœ¬æç¤º",
            "one_shot": "å–®æ¨£æœ¬æç¤º",
            "few_shot": "å°‘æ¨£æœ¬æç¤º",
            "cot": "æ€ç¶­éˆæç¤º",
            "zero_shot_cot": "é›¶æ¨£æœ¬æ€ç¶­éˆæç¤º", 
            "step_back": "å›é€€æ€è€ƒæç¤º",
            "react": "æ¨ç†èˆ‡è¡Œå‹•æç¤º",
            "role": "è§’è‰²æ‰®æ¼”æç¤º",
            "other": "å…¶ä»–é¡å‹æç¤º"
        },
        "save_prompt": "ğŸ’¾ ä¿å­˜æç¤º",
        "load_prompt": "ğŸ“ è¼‰å…¥æç¤º",
        "load_original": "ğŸ“„ è¼‰å…¥åŸå§‹",
        "load_optimized": "âœ¨ è¼‰å…¥å„ªåŒ–",
        "prompt_library": "æç¤ºè©åº«",
        "save_name": "æç¤ºåç¨±",
        "save_tags": "æ¨™ç±¤ (ç”¨é€—è™Ÿåˆ†éš”)",
        "save_success": "æç¤ºå·²ä¿å­˜ï¼",
        "save_error": "ä¿å­˜å¤±æ•—",
        "load_success": "æç¤ºå·²è¼‰å…¥ï¼",
        "no_saved_prompts": "æš«ç„¡ä¿å­˜çš„æç¤º",
        "delete_prompt": "ğŸ—‘ï¸ åˆªé™¤",
        "confirm_delete": "ç¢ºèªåˆªé™¤æ­¤æç¤ºï¼Ÿ",
        "search_prompts": "æœå°‹æç¤ºè©",
        "prompt_name": "æç¤ºåç¨±",
        "created_at": "å‰µå»ºæ™‚é–“",
        "copy_prompt": "ğŸ“‹ è¤‡è£½æç¤º",
        "export_prompts": "ğŸ“¤ åŒ¯å‡º",
        "import_prompts": "ğŸ“¥ åŒ¯å…¥",
        "export_success": "åŒ¯å‡ºæˆåŠŸï¼",
        "import_success": "åŒ¯å…¥æˆåŠŸï¼å·²åŒ¯å…¥ {imported} ç­†ï¼Œè·³é {skipped} ç­†ï¼ŒéŒ¯èª¤ {errors} ç­†",
        "import_error": "åŒ¯å…¥å¤±æ•—ï¼š{error}",
        "import_file_label": "é¸æ“‡ JSON æª”æ¡ˆ",
        "overwrite_existing": "è¦†è“‹å·²å­˜åœ¨çš„æç¤ºè©",
        "local_storage_notice": "âš ï¸ è³‡æ–™å„²å­˜åœ¨ç€è¦½å™¨ä¸­ï¼Œè«‹å®šæœŸåŒ¯å‡ºä»¥æ°¸ä¹…ä¿å­˜",
        "specific_model": "å…·é«”æ¨¡å‹",
        "gemini_api_key_note": "éœ€è¦è¨­ç½® GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸",
        "gemini_api_key_input": "Gemini API Key",
        "gemini_api_key_placeholder": "è¼¸å…¥æ‚¨çš„ Gemini API Key (å¯é¸,æœƒè¦†å¯«ç’°å¢ƒè®Šæ•¸)",
        "gemini_api_key_help": "åœ¨æ­¤è¼¸å…¥çš„ API Key æœƒè¦†å¯« .env ä¸­çš„è¨­å®š",
        "gemini_api_key_configured": "âœ… API Key å·²è¨­å®š",
        "gemini_api_key_edit": "âœï¸ ç·¨è¼¯ API Key",
        "gemini_api_key_confirm": "âœ… ç¢ºèª",
        "gemini_api_key_cancel": "âŒ å–æ¶ˆ",
        "gemini_api_key_get_link": "ğŸ”‘ [å–å¾— Gemini API Key](https://aistudio.google.com/app/apikey)",
        "vertex_project_note": "éœ€è¦è¨­ç½® GOOGLE_CLOUD_PROJECT ç’°å¢ƒè®Šæ•¸å’Œ Google Cloud èªè­‰",
        # UI æ¨¡å¼åˆ‡æ›
        "ui_mode_settings": "ä»‹é¢æ¨¡å¼",
        "ui_mode_label": "é¸æ“‡ UI æ¨¡å¼",
        "conversation_mode": "å°è©±æ¨¡å¼",
        "classic_mode": "å‚³çµ±æ¨¡å¼",
    },
    "en": {  # è‹±æ–‡
        "app_title": "AI Prompt Engineering Consultant",
        "initial_prompt_header": "Enter Your Initial Prompt",
        "initial_prompt_label": "Enter the prompt you want to optimize",
        "analyze_button": "Analyze Prompt",
        "please_input": "Please enter a prompt first",
        "improvement_header": "Let's Improve Your Prompt",
        "generate_button": "Generate Optimized Prompt",
        "result_header": "Optimized Prompt Result",
        "original_prompt": "Original Prompt",
        "enhanced_prompt": "Enhanced Prompt",
        "copy_text": "Ready to copy and use",
        "improvement_description": "Improvement Description",
        "optimize_again": "Optimize Again",
        "restart": "Start Over",
        "test_connection": "Test Connection",
        "connection_success": "Connection Successful",
        "connection_error": "Connection Error",
        "aws_settings": "LLM Settings",
        "select_llm": "Select LLM Model",
        "select_preset": "Select Parameter Preset",
        "select_region": "Select AWS Region",
        "model_params": "Model Parameters",
        "max_tokens": "Maximum Output Tokens",
        "system_prompt": "System Prompt",
        "user_prompt": "User Prompt",
        "estimated_tokens": "Estimated Input Tokens",
        "execute_button": "Execute",
        "processing": "Processing...",
        "output_result": "Output Result",
        "usage_info": "Usage Information",
        "input_tokens": "Input Tokens",
        "output_tokens": "Output Tokens",
        "total_tokens": "Total Tokens",
        "processing_time": "Processing Time",
        "api_error": "Error calling API",
        "custom_preset": "Custom",
        "prompt_type": "Prompt Type",
        "prompt_types": {
            "zero_shot": "Zero-Shot Prompt",
            "one_shot": "One-Shot Prompt",
            "few_shot": "Few-Shot Prompt",
            "cot": "Chain of Thought Prompt",
            "zero_shot_cot": "Zero-Shot Chain of Thought Prompt",
            "step_back": "Step-Back Prompt",
            "react": "ReAct (Reason+Act) Prompt",
            "role": "Role-Playing Prompt",
            "other": "Other Prompt Type"
        },
        "save_prompt": "ğŸ’¾ Save Prompt",
        "load_prompt": "ğŸ“ Load Prompt",
        "load_original": "ğŸ“„ Load Original",
        "load_optimized": "âœ¨ Load Optimized",
        "prompt_library": "Prompt Library",
        "save_name": "Prompt Name",
        "save_tags": "Tags (comma separated)",
        "save_success": "Prompt saved successfully!",
        "save_error": "Save failed",
        "load_success": "Prompt loaded successfully!",
        "no_saved_prompts": "No saved prompts",
        "delete_prompt": "ğŸ—‘ï¸ Delete",
        "confirm_delete": "Confirm delete this prompt?",
        "search_prompts": "Search prompts",
        "prompt_name": "Prompt Name",
        "created_at": "Created At",
        "copy_prompt": "ğŸ“‹ Copy Prompt",
        "export_prompts": "ğŸ“¤ Export",
        "import_prompts": "ğŸ“¥ Import",
        "export_success": "Export successful!",
        "import_success": "Import successful! Imported {imported}, skipped {skipped}, errors {errors}",
        "import_error": "Import failed: {error}",
        "import_file_label": "Select JSON file",
        "overwrite_existing": "Overwrite existing prompts",
        "local_storage_notice": "âš ï¸ Data is stored in browser. Export regularly for permanent backup",
        "specific_model": "Specific Model",
        "gemini_api_key_note": "Requires GEMINI_API_KEY environment variable",
        "gemini_api_key_input": "Gemini API Key",
        "gemini_api_key_placeholder": "Enter your Gemini API Key (optional, overrides environment variable)",
        "gemini_api_key_help": "API Key entered here will override the .env setting",
        "gemini_api_key_configured": "âœ… API Key Configured",
        "gemini_api_key_edit": "âœï¸ Edit API Key",
        "gemini_api_key_confirm": "âœ… Confirm",
        "gemini_api_key_cancel": "âŒ Cancel",
        "gemini_api_key_get_link": "ğŸ”‘ [Get Gemini API Key](https://aistudio.google.com/app/apikey)",
        "vertex_project_note": "Requires GOOGLE_CLOUD_PROJECT environment variable and Google Cloud authentication",
        # UI æ¨¡å¼åˆ‡æ›
        "ui_mode_settings": "Interface Mode",
        "ui_mode_label": "Select UI Mode",
        "conversation_mode": "Conversation",
        "classic_mode": "Classic",
    },
    "ja": {  # æ—¥æ–‡
        "app_title": "AI ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ",
        "initial_prompt_header": "åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        "initial_prompt_label": "æœ€é©åŒ–ã—ãŸã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã“ã“ã«å…¥åŠ›ã—ã¦ãã ã•ã„",
        "analyze_button": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ†æ",
        "please_input": "æœ€åˆã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        "improvement_header": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ”¹å–„ã—ã¾ã—ã‚‡ã†",
        "generate_button": "æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ",
        "result_header": "æœ€é©åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµæœ",
        "original_prompt": "å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        "enhanced_prompt": "å¼·åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        "copy_text": "ã‚³ãƒ”ãƒ¼ã—ã¦ä½¿ç”¨ã§ãã¾ã™",
        "improvement_description": "æ”¹å–„ã®èª¬æ˜",
        "optimize_again": "å†åº¦æœ€é©åŒ–",
        "restart": "æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã™",
        "test_connection": "æ¥ç¶šãƒ†ã‚¹ãƒˆ",
        "connection_success": "æ¥ç¶šæˆåŠŸ",
        "connection_error": "æ¥ç¶šã‚¨ãƒ©ãƒ¼",
        "aws_settings": "LLMè¨­å®š",
        "select_llm": "LLMãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
        "select_preset": "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é¸æŠ",
        "select_region": "AWSãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’é¸æŠ",
        "model_params": "ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿",
        "max_tokens": "æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
        "system_prompt": "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        "user_prompt": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        "estimated_tokens": "æ¨å®šå…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
        "execute_button": "å®Ÿè¡Œ",
        "processing": "å‡¦ç†ä¸­...",
        "output_result": "å‡ºåŠ›çµæœ",
        "usage_info": "ä½¿ç”¨æƒ…å ±",
        "input_tokens": "å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³",
        "output_tokens": "å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³",
        "total_tokens": "åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³",
        "processing_time": "å‡¦ç†æ™‚é–“",
        "api_error": "APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼",
        "custom_preset": "ã‚«ã‚¹ã‚¿ãƒ ",
        "prompt_type": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—",
        "prompt_types": {
            "zero_shot": "ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "one_shot": "ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "few_shot": "ãƒ•ãƒ¥ãƒ¼ã‚·ãƒ§ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "cot": "æ€è€ƒã®é€£é–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "zero_shot_cot": "ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ€è€ƒã®é€£é–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "step_back": "ã‚¹ãƒ†ãƒƒãƒ—ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "react": "æ¨è«–ã¨è¡Œå‹•ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "role": "ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "other": "ãã®ä»–ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
        },
        "save_prompt": "ğŸ’¾ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿å­˜",
        "load_prompt": "ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã¿",
        "load_original": "ğŸ“„ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚’èª­ã¿è¾¼ã¿",
        "load_optimized": "âœ¨ æœ€é©åŒ–ç‰ˆã‚’èª­ã¿è¾¼ã¿",
        "prompt_library": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª",
        "save_name": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå",
        "save_tags": "ã‚¿ã‚° (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)",
        "save_success": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒä¿å­˜ã•ã‚Œã¾ã—ãŸï¼",
        "save_error": "ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ",
        "load_success": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒèª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸï¼",
        "no_saved_prompts": "ä¿å­˜ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã‚ã‚Šã¾ã›ã‚“",
        "delete_prompt": "ğŸ—‘ï¸ å‰Šé™¤",
        "confirm_delete": "ã“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ",
        "search_prompts": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¤œç´¢",
        "prompt_name": "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå",
        "created_at": "ä½œæˆæ—¥æ™‚",
        "copy_prompt": "ğŸ“‹ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚³ãƒ”ãƒ¼",
        "export_prompts": "ğŸ“¤ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ",
        "import_prompts": "ğŸ“¥ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ",
        "export_success": "ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæˆåŠŸï¼",
        "import_success": "ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸï¼{imported}ä»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€{skipped}ä»¶ã‚¹ã‚­ãƒƒãƒ—ã€{errors}ä»¶ã‚¨ãƒ©ãƒ¼",
        "import_error": "ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—ï¼š{error}",
        "import_file_label": "JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
        "overwrite_existing": "æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸Šæ›¸ã",
        "local_storage_notice": "âš ï¸ ãƒ‡ãƒ¼ã‚¿ã¯ãƒ–ãƒ©ã‚¦ã‚¶ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚å®šæœŸçš„ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„",
        "specific_model": "ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«",
        "gemini_api_key_note": "GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ãŒå¿…è¦ã§ã™",
        "gemini_api_key_input": "Gemini API Key",
        "gemini_api_key_placeholder": "Gemini API Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ç’°å¢ƒå¤‰æ•°ã‚’ä¸Šæ›¸ãï¼‰",
        "gemini_api_key_help": "ã“ã“ã«å…¥åŠ›ã•ã‚ŒãŸAPI Keyã¯.envã®è¨­å®šã‚’ä¸Šæ›¸ãã—ã¾ã™",
        "gemini_api_key_configured": "âœ… API Keyè¨­å®šæ¸ˆã¿",
        "gemini_api_key_edit": "âœï¸ API Keyã‚’ç·¨é›†",
        "gemini_api_key_confirm": "âœ… ç¢ºèª",
        "gemini_api_key_cancel": "âŒ ã‚­ãƒ£ãƒ³ã‚»ãƒ«",
        "gemini_api_key_get_link": "ğŸ”‘ [Gemini API Keyã‚’å–å¾—](https://aistudio.google.com/app/apikey)",
        "vertex_project_note": "GOOGLE_CLOUD_PROJECTç’°å¢ƒå¤‰æ•°ã¨Google Cloudèªè¨¼ãŒå¿…è¦ã§ã™",
        # UI æ¨¡å¼åˆ‡æ›
        "ui_mode_settings": "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰",
        "ui_mode_label": "UIãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ",
        "conversation_mode": "ä¼šè©±ãƒ¢ãƒ¼ãƒ‰",
        "classic_mode": "ã‚¯ãƒ©ã‚·ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰",
    }
}

# å‹•æ…‹åˆä½µå°è©±å¼ UI çš„ç¿»è­¯
ui_translations = get_conversation_ui_translations()
for lang in translations:
    if lang in ui_translations:
        translations[lang].update(ui_translations[lang])

# ç²å–ç¿»è­¯
def t(key):
    return translations[st.session_state.language].get(key, key)

# ç²å–æ¨¡å‹çš„ context window é™åˆ¶
def get_context_window_limit(llm_type: str) -> int:
    """
    æ ¹æ“š LLM é¡å‹è¿”å›åˆé©çš„ context window é™åˆ¶

    Args:
        llm_type: LLM é¡å‹

    Returns:
        Context window é™åˆ¶ï¼ˆtokensï¼‰
    """
    context_limits = {
        "claude": 200000,      # Claude 3.5 Sonnet
        "gemini": 1000000,     # Gemini 1.5 Flash/Pro
        "gemini-vertex": 1000000
    }
    return context_limits.get(llm_type, 200000)  # é è¨­ 200k

# åˆå§‹åŒ–æœƒè©±ç‹€æ…‹
def initialize_session_state():
    # è¼‰å…¥é…ç½®
    config = get_default_config_loader()

    # è®€å– dev_mode è¨­å®š (Streamlit Secrets å„ªå…ˆï¼Œç„¶å¾Œ config.yaml)
    if 'dev_mode' not in st.session_state:
        try:
            if "dev_mode" in st.secrets:
                st.session_state.dev_mode = st.secrets["dev_mode"]
            else:
                st.session_state.dev_mode = config.get('app.dev_mode', True)
        except Exception:
            # No secrets.toml file exists (local development)
            st.session_state.dev_mode = config.get('app.dev_mode', True)

    # Provider åç¨±å°æ‡‰è¡¨
    provider_display_map = {
        "gemini": "Gemini (API Key)",
        "gemini-vertex": "Gemini (Vertex AI)",
        "claude": "Claude (AWS Bedrock)"
    }

    if 'language' not in st.session_state:
        st.session_state.language = config.get_default_language()

    # LLM æ¨¡å‹é¸æ“‡ - å¾é…ç½®æª”æ¡ˆè®€å–é è¨­å€¼
    if 'llm_provider' not in st.session_state:
        default_provider = config.get_default_provider()
        st.session_state.llm_provider = provider_display_map.get(default_provider, "Gemini (API Key)")

    if 'llm_type' not in st.session_state:
        st.session_state.llm_type = config.get_default_provider()

    if 'llm_model' not in st.session_state:
        provider = config.get_default_provider()
        llm_config = config.get_llm_config(provider) or {}  # é˜²ç¦¦ None
        st.session_state.llm_model = llm_config.get('model', 'gemini-3-flash-preview')

    if 'aws_region' not in st.session_state:
        claude_config = config.get_llm_config('claude') or {}  # é˜²ç¦¦ None
        st.session_state.aws_region = claude_config.get('region', 'us-west-2')

    if 'gemini_api_key' not in st.session_state:
        st.session_state.gemini_api_key = ""  # ç”¨æˆ¶ç¢ºèªå¾Œçš„ API Key
    if 'gemini_api_key_temp' not in st.session_state:
        st.session_state.gemini_api_key_temp = ""  # è‡¨æ™‚è¼¸å…¥çš„ API Key (æœªç¢ºèª)
    if 'show_gemini_api_key_input' not in st.session_state:
        # å¦‚æœé‚„æ²’æœ‰è¨­å®š API Key,é è¨­é¡¯ç¤ºè¼¸å…¥æ¡†
        st.session_state.show_gemini_api_key_input = (st.session_state.gemini_api_key == "")

    # å›ºå®šä½¿ç”¨æœ€é©åˆ Prompt åˆ†æçš„åƒæ•¸
    # ä¸éœ€è¦ session_state å­˜å„²,ç›´æ¥åœ¨å‡½æ•¸ä¸­ä½¿ç”¨å›ºå®šå€¼

    # åˆå§‹åŒ–è³‡æ–™åº« - æ ¹æ“š dev_mode é¸æ“‡å„²å­˜æ–¹å¼
    if 'prompt_db' not in st.session_state:
        if st.session_state.dev_mode:
            # é–‹ç™¼æ¨¡å¼ï¼šä½¿ç”¨ SQLite è³‡æ–™åº«
            db_path = config.get('app.database.path', 'prompts.db')
            st.session_state.prompt_db = PromptDatabase(db_path)
        else:
            # ä¸Šç·šæ¨¡å¼ï¼šä½¿ç”¨ç€è¦½å™¨ LocalStorage
            st.session_state.prompt_db = LocalStoragePromptDB()

    # åˆå§‹åŒ–å°è©±æ¨¡å¼ç›¸é—œç‹€æ…‹
    if 'conversation_mode' not in st.session_state:
        st.session_state.conversation_mode = True  # é è¨­å•Ÿç”¨å°è©±æ¨¡å¼

    if 'current_session' not in st.session_state:
        # æ ¹æ“š LLM é¡å‹è¨­å®šåˆé©çš„ context window é™åˆ¶
        llm_type = st.session_state.get('llm_type', 'gemini')
        context_limit = get_context_window_limit(llm_type)
        st.session_state.current_session = create_new_session(context_limit=context_limit)
    else:
        # å¦‚æœ LLM é¡å‹æ”¹è®Šï¼Œæ›´æ–° context limit
        current_llm_type = st.session_state.get('llm_type', 'gemini')
        expected_limit = get_context_window_limit(current_llm_type)
        if st.session_state.current_session.context_window_limit != expected_limit:
            st.session_state.current_session.context_window_limit = expected_limit

    # å‘å¾Œç›¸å®¹ï¼šä¿ç•™ç¾æœ‰çš„ current_stageï¼ˆç”¨æ–¼ classic æ¨¡å¼ï¼‰
    if 'current_stage' not in st.session_state:
        st.session_state.current_stage = "initial"

    # åˆå§‹åŒ–å°è©±å¼ UI è§¸ç™¼å™¨
    if 'trigger_optimization' not in st.session_state:
        st.session_state.trigger_optimization = False
    if 'trigger_iterate' not in st.session_state:
        st.session_state.trigger_iterate = False
    if 'pending_responses' not in st.session_state:
        st.session_state.pending_responses = {}
    if 'is_processing' not in st.session_state:
        st.session_state.is_processing = False



# å‰µå»º LLM å¯¦ä¾‹
def create_llm():
    llm_type = st.session_state.llm_type
    
    if llm_type == "claude":
        return LLMFactory.create_llm(
            llm_type,
            region=st.session_state.aws_region
        )
    elif llm_type == "gemini":
        # å¦‚æœç”¨æˆ¶è¼¸å…¥äº† API Key,ä½¿ç”¨å®ƒ;å¦å‰‡ä½¿ç”¨ç’°å¢ƒè®Šæ•¸
        kwargs = {"model": st.session_state.llm_model}
        if st.session_state.gemini_api_key:
            kwargs["api_key"] = st.session_state.gemini_api_key
        return LLMFactory.create_llm(llm_type, **kwargs)
    elif llm_type == "gemini-vertex":
        return LLMFactory.create_llm(
            llm_type,
            model=st.session_state.llm_model
        )
    else:
        # é»˜èªè¿”å› Gemini
        kwargs = {"model": st.session_state.llm_model}
        if st.session_state.gemini_api_key:
            kwargs["api_key"] = st.session_state.gemini_api_key
        return LLMFactory.create_llm("gemini", **kwargs)


# ç²å–å›ºå®šçš„æœ€ä½³åˆ†æåƒæ•¸
def get_current_params():
    """
    è¿”å›å›ºå®šçš„æœ€ä½³ Prompt åˆ†æåƒæ•¸
    Temperature=0.2 ç¢ºä¿åˆ†æçµæœç©©å®šä¸€è‡´
    """
    return {
        "temperature": 0.2,  # ä½æº«åº¦ç¢ºä¿ç©©å®šã€å¯é‡è¤‡çš„åˆ†æ
        "top_p": 0.9,        # é©ä¸­çš„é¸æ“‡ç¯„åœ
        "top_k": 40,         # æ¨™æº–è¨­ç½®
        "max_tokens": 4096   # è¶³å¤ çš„è¼¸å‡ºç©ºé–“
    }

# é¡¯ç¤ºå´é‚Šæ¬„
def show_sidebar():
    # UI æ¨¡å¼åˆ‡æ›ï¼ˆæ‰€æœ‰ç”¨æˆ¶å¯ç”¨ï¼‰
    st.sidebar.markdown("### âš™ï¸ " + t("ui_mode_settings"))
    mode = st.sidebar.radio(
        t("ui_mode_label"),
        options=[t("conversation_mode"), t("classic_mode")],
        index=0 if st.session_state.conversation_mode else 1,
        horizontal=True
    )
    new_mode = (mode == t("conversation_mode"))
    if new_mode != st.session_state.conversation_mode:
        st.session_state.conversation_mode = new_mode
        st.rerun()

    # å°è©±æ¨¡å¼ï¼šé¡¯ç¤ºæ–°å°è©±æŒ‰éˆ•
    if st.session_state.conversation_mode:
        render_new_conversation_button(t)

    st.sidebar.markdown("---")

    # é–‹ç™¼æ¨¡å¼ï¼šé¡¯ç¤ºå®Œæ•´ LLM è¨­å®š
    if st.session_state.dev_mode:
        st.sidebar.header(t("aws_settings"))

        # LLM æ¨¡å‹é¸æ“‡
        available_models = LLMFactory.get_available_models()

        # æä¾›è€…é¸æ“‡
        selected_provider = st.sidebar.selectbox(
            t("select_llm"),
            list(available_models.keys()),
            index=list(available_models.keys()).index(st.session_state.llm_provider) if st.session_state.llm_provider in available_models else 0
        )

        # æ›´æ–° session state
        if selected_provider != st.session_state.llm_provider:
            st.session_state.llm_provider = selected_provider
            st.session_state.llm_type = available_models[selected_provider]["type"]
            st.session_state.llm_model = available_models[selected_provider]["models"][0]  # é»˜èªç¬¬ä¸€å€‹æ¨¡å‹

        # æ¨¡å‹é¸æ“‡
        selected_model = st.sidebar.selectbox(
            t("specific_model"),
            available_models[selected_provider]["models"],
            index=available_models[selected_provider]["models"].index(st.session_state.llm_model) if st.session_state.llm_model in available_models[selected_provider]["models"] else 0
        )
        st.session_state.llm_model = selected_model

        # é¡¯ç¤ºèªè­‰éœ€æ±‚æç¤ºå’Œé…ç½®
        if st.session_state.llm_type == "gemini":
            st.sidebar.info(t("gemini_api_key_note"))

            # æ ¹æ“šç‹€æ…‹é¡¯ç¤ºè¼¸å…¥æ¡†æˆ–ç·¨è¼¯æŒ‰éˆ•
            if st.session_state.show_gemini_api_key_input:
                # é¡¯ç¤ºè¼¸å…¥æ¡†
                gemini_api_key_input = st.sidebar.text_input(
                    t("gemini_api_key_input"),
                    value=st.session_state.gemini_api_key_temp if st.session_state.gemini_api_key_temp else st.session_state.gemini_api_key,
                    type="password",
                    placeholder=t("gemini_api_key_placeholder"),
                    help=t("gemini_api_key_help"),
                    key="gemini_api_key_input_field"
                )

                # å°‡è¼¸å…¥å­˜å„²åˆ°è‡¨æ™‚è®Šæ•¸
                st.session_state.gemini_api_key_temp = gemini_api_key_input

                # æ·»åŠ ç¢ºèªå’Œå–æ¶ˆæŒ‰éˆ•
                col1, col2 = st.sidebar.columns(2)
                with col1:
                    if st.button(t("gemini_api_key_confirm"), key="confirm_api_key", use_container_width=True):
                        # ç¢ºèªå¾Œä¿å­˜åˆ°æ­£å¼è®Šæ•¸
                        st.session_state.gemini_api_key = st.session_state.gemini_api_key_temp
                        st.session_state.show_gemini_api_key_input = False
                        st.session_state.gemini_api_key_temp = ""  # æ¸…ç©ºè‡¨æ™‚è®Šæ•¸
                        st.rerun()
                with col2:
                    if st.button(t("gemini_api_key_cancel"), key="cancel_api_key", use_container_width=True):
                        # å–æ¶ˆç·¨è¼¯,æ¸…ç©ºè‡¨æ™‚è®Šæ•¸
                        st.session_state.gemini_api_key_temp = ""
                        # å¦‚æœæœ‰å·²ä¿å­˜çš„ API Key,éš±è—è¼¸å…¥æ¡†
                        if st.session_state.gemini_api_key:
                            st.session_state.show_gemini_api_key_input = False
                        st.rerun()
            else:
                # é¡¯ç¤ºå·²é…ç½®æç¤ºå’Œç·¨è¼¯æŒ‰éˆ•
                st.sidebar.success(t("gemini_api_key_configured"))
                if st.sidebar.button(t("gemini_api_key_edit"), key="edit_api_key"):
                    st.session_state.show_gemini_api_key_input = True
                    st.rerun()

            # é¡¯ç¤ºå–å¾— API Key çš„é€£çµï¼ˆçµ±ä¸€è™•ç†ï¼Œé¿å…é‡è¤‡ï¼‰
            st.sidebar.markdown(t("gemini_api_key_get_link"))

        elif st.session_state.llm_type == "gemini-vertex":
            st.sidebar.info(t("vertex_project_note"))

        # å¦‚æœæ˜¯ Claude (AWS Bedrock)ï¼Œé¡¯ç¤ºå€åŸŸé¸æ“‡
        if st.session_state.llm_type == "claude":
            aws_regions = ["us-east-1", "us-west-2", "eu-west-1", "ap-southeast-1"]
            selected_region = st.sidebar.selectbox(
                t("select_region"),
                aws_regions,
                index=aws_regions.index(st.session_state.aws_region) if st.session_state.aws_region in aws_regions else 1
            )
            st.session_state.aws_region = selected_region

        # å›ºå®šä½¿ç”¨æœ€é©åˆ Prompt åˆ†æçš„åƒæ•¸ (ä¸é¡¯ç¤ºåƒæ•¸èª¿æ•´é¸é …)
        # Temperature=0.2 ç¢ºä¿åˆ†æçµæœç©©å®šä¸€è‡´
        # ç”¨æˆ¶ç„¡éœ€èª¿æ•´é€™äº›åƒæ•¸,ç³»çµ±æœƒè‡ªå‹•ä½¿ç”¨æœ€ä½³è¨­ç½®

        # é€£æ¥æ¸¬è©¦
        st.sidebar.header(t("test_connection"))
        if st.sidebar.button(t("test_connection")):
            with st.sidebar:
                llm = create_llm()
                is_connected, message = llm.check_connection()
                if is_connected:
                    st.success(message)
                else:
                    st.error(message)

    # æç¤ºè©åº«ç®¡ç†ï¼ˆæ‰€æœ‰æ¨¡å¼éƒ½é¡¯ç¤ºï¼‰
    st.sidebar.header(t("prompt_library"))

    # ä¸Šç·šæ¨¡å¼ï¼šé¡¯ç¤º LocalStorage æç¤º
    if not st.session_state.dev_mode:
        st.sidebar.warning(t("local_storage_notice"))

    show_prompt_library_sidebar()


# å¿«å–åŒ¯å‡ºè³‡æ–™ä»¥é¿å…æ¯æ¬¡æ¸²æŸ“éƒ½é‡æ–°ç”Ÿæˆ
@st.cache_data(ttl=60)  # Cache for 60 seconds
def get_cached_export_data(_db, _cache_key: str) -> str:
    """Cache export data to avoid regenerating on every render"""
    return _db.export_prompts()


# é¡¯ç¤ºæç¤ºè©åº«å´é‚Šæ¬„
def show_prompt_library_sidebar():
    """é¡¯ç¤ºæç¤ºè©åº«ç®¡ç†ç•Œé¢"""
    db = st.session_state.prompt_db

    # ä½¿ç”¨ cache key ä¾†åœ¨è³‡æ–™è®Šæ›´æ™‚é‡æ–°ç”ŸæˆåŒ¯å‡ºè³‡æ–™
    cache_key = st.session_state.get('export_cache_key', 'initial')

    # åŒ¯å‡º/åŒ¯å…¥æŒ‰éˆ•
    col_exp, col_imp = st.sidebar.columns(2)
    with col_exp:
        # åŒ¯å‡ºæŒ‰éˆ• - ä½¿ç”¨å¿«å–çš„è³‡æ–™
        export_data = get_cached_export_data(db, cache_key)
        st.download_button(
            label=t("export_prompts"),
            data=export_data,
            file_name="prompts_backup.json",
            mime="application/json",
            use_container_width=True
        )

    with col_imp:
        # åŒ¯å…¥æŒ‰éˆ• - ä½¿ç”¨ popover é¡¯ç¤ºä¸Šå‚³ç•Œé¢
        with st.popover(t("import_prompts"), use_container_width=True):
            uploaded_file = st.file_uploader(
                t("import_file_label"),
                type=['json'],
                key="import_file"
            )
            overwrite = st.checkbox(t("overwrite_existing"), value=False)

            if uploaded_file is not None:
                if st.button("âœ… " + t("import_prompts"), key="do_import"):
                    try:
                        # Handle UTF-8 encoding with error handling
                        raw_data = uploaded_file.read()
                        try:
                            json_data = raw_data.decode('utf-8')
                        except UnicodeDecodeError:
                            # Fallback to utf-8 with error replacement
                            json_data = raw_data.decode('utf-8', errors='replace')
                            st.warning("âš ï¸ Some characters may have been replaced due to encoding issues")
                    except Exception as e:
                        st.error(t("import_error").format(error=f"File read error: {str(e)}"))
                        json_data = None

                    if json_data:
                        result = db.import_prompts(json_data, overwrite=overwrite)

                        if result.get("success"):
                            # Invalidate export cache
                            st.session_state.export_cache_key = str(time.time())
                            st.success(t("import_success").format(
                                imported=result["imported"],
                                skipped=result["skipped"],
                                errors=result["errors"]
                            ))
                            st.rerun()
                        else:
                            st.error(t("import_error").format(error=result.get("error", "Unknown")))

    # æœç´¢æ¡†
    search_query = st.sidebar.text_input(t("search_prompts"), key="search_prompts")

    # è¼‰å…¥æç¤ºè©
    if search_query:
        prompts = db.search_prompts(search_query, st.session_state.language)
    else:
        prompts = db.load_prompts(limit=20)
    
    if prompts:
        # é¡¯ç¤ºæç¤ºè©åˆ—è¡¨
        for prompt in prompts:
            with st.sidebar.expander(f"ğŸ“ {prompt['name'][:30]}..."):
                st.write(f"**{t('created_at')}:** {prompt['created_at'][:10]}")
                if prompt['tags']:
                    st.write(f"**Tags:** {', '.join(prompt['tags'])}")
                
                # é è¦½å€åŸŸ
                preview_tab1, preview_tab2 = st.tabs(["ğŸ“„ åŸå§‹", "âœ¨ å„ªåŒ–"])
                with preview_tab1:
                    st.text_area("åŸå§‹æç¤º", prompt['original_prompt'][:100] + "...", height=80, disabled=True, key=f"orig_{prompt['id']}")
                with preview_tab2:
                    st.text_area("å„ªåŒ–æç¤º", prompt['optimized_prompt'][:100] + "...", height=80, disabled=True, key=f"opt_{prompt['id']}")
                
                # è¼‰å…¥æŒ‰éˆ•çµ„
                col1, col2 = st.columns(2)
                with col1:
                    if st.button(t("load_original"), key=f"load_orig_{prompt['id']}"):
                        # è¼‰å…¥åŸå§‹æç¤º
                        st.session_state.initial_prompt = prompt['original_prompt']
                        st.session_state.current_stage = "initial"
                        st.success(f"âœ… {t('load_success')} (åŸå§‹)")
                        st.rerun()
                
                with col2:
                    if st.button(t("load_optimized"), key=f"load_opt_{prompt['id']}"):
                        # è¼‰å…¥å„ªåŒ–æç¤º
                        st.session_state.initial_prompt = prompt['optimized_prompt']
                        st.session_state.current_stage = "initial"
                        st.success(f"âœ… {t('load_success')} (å„ªåŒ–)")
                        st.rerun()
                
                # åˆªé™¤æŒ‰éˆ•
                if st.button(t("delete_prompt"), key=f"del_{prompt['id']}", use_container_width=True):
                    if db.delete_prompt(prompt['id']):
                        # Invalidate export cache
                        st.session_state.export_cache_key = str(time.time())
                        st.success("å·²åˆªé™¤")
                        st.rerun()
    else:
        st.sidebar.info(t("no_saved_prompts"))


# ä¿å­˜æç¤ºå°è©±æ¡†
def show_save_prompt_dialog(original_prompt, optimized_prompt, analysis_scores=None):
    """é¡¯ç¤ºä¿å­˜æç¤ºçš„å°è©±æ¡†"""
    with st.expander(t("save_prompt"), expanded=False):
        # ä½¿ç”¨ form ä¾†é¿å… session state å•é¡Œ
        with st.form("save_prompt_form"):
            save_name = st.text_input(t("save_name"))
            save_tags = st.text_input(t("save_tags"))
            
            if st.form_submit_button(t("save_prompt")):
                if save_name:
                    try:
                        # è™•ç†æ¨™ç±¤
                        tags = [tag.strip() for tag in save_tags.split(",") if tag.strip()] if save_tags else []
                        
                        # ä¿å­˜åˆ°è³‡æ–™åº«
                        prompt_id = st.session_state.prompt_db.save_prompt(
                            name=save_name,
                            original_prompt=original_prompt,
                            optimized_prompt=optimized_prompt,
                            analysis_scores=analysis_scores,
                            tags=tags,
                            language=st.session_state.language
                        )

                        # Invalidate export cache
                        st.session_state.export_cache_key = str(time.time())
                        st.success(t("save_success"))
                        st.rerun()  # é‡æ–°é‹è¡Œä»¥æ¸…ç©ºè¡¨å–®
                        
                    except Exception as e:
                        st.error(f"{t('save_error')}: {str(e)}")
                else:
                    st.warning("è«‹è¼¸å…¥æç¤ºåç¨±")


# é¡¯ç¤ºæç¤ºå„ªåŒ–ç•Œé¢
def show_optimize_ui():
    st.header(t("app_title"))
    
    # å¦‚æœè™•æ–¼èµ·å§‹éšæ®µæˆ–é‡æ–°é–‹å§‹
    if not hasattr(st.session_state, 'current_stage') or st.session_state.current_stage == "initial":
        st.header(t("initial_prompt_header"))
        # ä½¿ç”¨ session state ä¸­çš„ initial_prompt ä½œç‚ºé è¨­å€¼
        default_value = st.session_state.get('initial_prompt', '')
        initial_prompt = st.text_area(t("initial_prompt_label"), value=default_value, height=200)
        
        # é¡¯ç¤ºè­˜åˆ¥çš„æç¤ºé¡å‹
        if initial_prompt:
            prompt_type = identify_prompt_type(initial_prompt)
            type_display = translations[st.session_state.language]["prompt_types"][prompt_type]
            st.info(f"**{t('prompt_type')}**: {type_display}")

        if st.button(t("analyze_button")):
            if initial_prompt:
                with st.spinner(t("processing")):
                    # å‰µå»ºè©•ä¼°å™¨ä¸¦åˆ†ææç¤º
                    llm_instance = create_llm()
                    evaluator = PromptEvaluator(llm_instance=llm_instance)
                    analysis = evaluator.analyze_prompt(initial_prompt, st.session_state.language)

                    # ä¿å­˜æç¤ºé¡å‹åˆ°æœƒè©±ç‹€æ…‹
                    st.session_state.prompt_type = identify_prompt_type(initial_prompt)
                    st.session_state.analysis = analysis
                    st.session_state.initial_prompt = initial_prompt
                    st.session_state.current_stage = "questions"
                    st.rerun()  # é‡æ–°é‹è¡Œä»¥é¡¯ç¤ºå•é¡Œ
            else:
                st.warning(t("please_input"))
    
    # å¦‚æœè™•æ–¼çµæœéšæ®µï¼Œé¡¯ç¤ºåŸå§‹å’Œå„ªåŒ–å¾Œçš„æç¤ºé¡å‹
    elif st.session_state.current_stage == "result":
        st.header(t("result_header"))
        
        result = st.session_state.optimization_result
        
        # é¡¯ç¤ºåŸå§‹æç¤ºåŠå…¶é¡å‹
        original_type = st.session_state.prompt_type
        original_type_display = translations[st.session_state.language]["prompt_types"][original_type]
        
        st.subheader(t("original_prompt"))
        st.caption(f"**{t('prompt_type')}**: {original_type_display}")
        st.text_area(t("original_prompt"), st.session_state.initial_prompt, height=150)
        
        # é¡¯ç¤ºå„ªåŒ–å¾Œçš„æç¤ºåŠå…¶é¡å‹
        enhanced_type = identify_prompt_type(result["enhanced_prompt"])
        enhanced_type_display = translations[st.session_state.language]["prompt_types"][enhanced_type]
        
        # é¡¯ç¤ºå„ªåŒ–å¾Œçš„æç¤ºæ¨™é¡Œå’Œè¤‡è£½æŒ‰éˆ•
        col_title, col_copy = st.columns([3, 1])
        with col_title:
            st.subheader(t("enhanced_prompt"))
        with col_copy:
            if st.button(t("copy_prompt"), key="copy_optimized_prompt"):
                st.toast("âœ… è«‹é¸æ“‡ä¸‹æ–¹æ–‡å­—æ¡†å…§å®¹é€²è¡Œè¤‡è£½", icon="ğŸ“‹")
        
        st.caption(f"**{t('prompt_type')}**: {enhanced_type_display}")
        st.text_area(t("copy_text"), result["enhanced_prompt"], height=200)
        
        st.subheader(t("improvement_description"))
        for improvement in result["improvements"]:
            st.markdown(f"- {improvement}")
        
        # ä¿å­˜æç¤ºåŠŸèƒ½
        show_save_prompt_dialog(
            st.session_state.initial_prompt, 
            result["enhanced_prompt"], 
            st.session_state.get('analysis', {})
        )
        
        # æä¾›é€²ä¸€æ­¥å„ªåŒ–é¸é …
        col1, col2 = st.columns(2)
        with col1:
            if st.button(t("optimize_again")):
                st.session_state.initial_prompt = result["enhanced_prompt"]
                st.session_state.prompt_type = enhanced_type
                st.session_state.current_stage = "questions"
                st.rerun()
        
        with col2:
            if st.button(t("restart")):
                for key in list(st.session_state.keys()):
                    if key not in ["language", "llm_type", "aws_region", "preset", "custom_params", "mode", "prompt_db"]:
                        if key in st.session_state:
                            del st.session_state[key]
                st.session_state.current_stage = "initial"
                st.rerun()

    # å¦‚æœè™•æ–¼å•é¡Œéšæ®µ
    elif st.session_state.current_stage == "questions":
        st.header(t("improvement_header"))
        
        analysis = st.session_state.analysis
        llm_instance = create_llm()
        evaluator = PromptEvaluator(llm_instance=llm_instance)
        questions = evaluator.generate_questions(analysis, st.session_state.language)
        
        user_responses = {}
        
        for i, question in enumerate(questions):
            if question["type"] == "reasoning":
                user_responses[question["type"]] = st.checkbox(question["question"])
            elif question.get("input_type") == "selectbox":
                # ä½¿ç”¨ä¸‹æ‹‰å¼é¸å–®
                options = question.get("options", [])
                labels = [opt["label"] for opt in options]
                keys = [opt["key"] for opt in options]
                default_key = question.get("default", "")
                default_index = keys.index(default_key) if default_key in keys else 0

                selected_label = st.selectbox(
                    question["question"],
                    labels,
                    index=default_index,
                    key=f"q_{i}"
                )
                # æ‰¾åˆ°å°æ‡‰çš„ key
                selected_index = labels.index(selected_label)
                user_responses[question["type"]] = keys[selected_index]
            else:
                user_responses[question["type"]] = st.text_input(f"{question['question']}", key=f"q_{i}")
        
        if st.button(t("generate_button")):
            # æ­¥é©Ÿ3ï¼šå„ªåŒ–æç¤º
            with st.spinner(t("processing")):
                optimization_result = evaluator.optimize_prompt(
                    st.session_state.initial_prompt, 
                    user_responses, 
                    analysis, 
                    st.session_state.language
                )
                st.session_state.optimization_result = optimization_result
                st.session_state.current_stage = "result"
                st.rerun()  # é‡æ–°é‹è¡Œä»¥é¡¯ç¤ºçµæœ
    



# æç¤ºé¡å‹è­˜åˆ¥å‡½æ•¸
def identify_prompt_type(prompt_text):
    """è­˜åˆ¥æç¤ºçš„é¡å‹"""
    prompt_types = {
        "zh_TW": {
            "zero_shot": "é›¶æ¨£æœ¬æç¤º",
            "one_shot": "å–®æ¨£æœ¬æç¤º",
            "few_shot": "å°‘æ¨£æœ¬æç¤º",
            "cot": "æ€ç¶­éˆæç¤º",
            "zero_shot_cot": "é›¶æ¨£æœ¬æ€ç¶­éˆæç¤º", 
            "step_back": "å›é€€æ€è€ƒæç¤º",
            "react": "æ¨ç†èˆ‡è¡Œå‹•æç¤º",
            "role": "è§’è‰²æ‰®æ¼”æç¤º",
            "other": "å…¶ä»–é¡å‹æç¤º"
        },
        "en": {
            "zero_shot": "Zero-Shot Prompt",
            "one_shot": "One-Shot Prompt",
            "few_shot": "Few-Shot Prompt",
            "cot": "Chain of Thought Prompt",
            "zero_shot_cot": "Zero-Shot Chain of Thought Prompt",
            "step_back": "Step-Back Prompt",
            "react": "ReAct (Reason+Act) Prompt",
            "role": "Role-Playing Prompt",
            "other": "Other Prompt Type"
        },
        "ja": {
            "zero_shot": "ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "one_shot": "ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "few_shot": "ãƒ•ãƒ¥ãƒ¼ã‚·ãƒ§ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "cot": "æ€è€ƒã®é€£é–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "zero_shot_cot": "ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ€è€ƒã®é€£é–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "step_back": "ã‚¹ãƒ†ãƒƒãƒ—ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "react": "æ¨è«–ã¨è¡Œå‹•ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "role": "ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            "other": "ãã®ä»–ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
        }
    }
    
    # æª¢æ¸¬æç¤ºé¡å‹çš„ç‰¹å¾µ
    prompt_lower = prompt_text.lower()
    
    # æª¢æ¸¬è§’è‰²æç¤º (å„ªå…ˆç´šæœ€é«˜)
    role_patterns = [
        "ä½ æ˜¯", "æ‰®æ¼”", "act as", "you are a", "role", 
        "ã‚ãªãŸã¯", "ã¨ã—ã¦è¡Œå‹•", "å½¹å‰²"
    ]
    for pattern in role_patterns:
        if pattern in prompt_lower:
            return "role"
    
    # æª¢æ¸¬ ReAct æç¤º
    react_patterns = [
        "æ€è€ƒ", "è¡Œå‹•", "è§€å¯Ÿ", "reason", "act", "observe", 
        "æ¨è«–", "è¡Œå‹•", "è¦³å¯Ÿ"
    ]
    react_count = sum(1 for pattern in react_patterns if pattern in prompt_lower)
    if react_count >= 2:  # è‡³å°‘åŒ…å«å…¶ä¸­å…©å€‹é—œéµè©
        return "react"
    
    # æª¢æ¸¬é›¶æ¨£æœ¬æ€ç¶­éˆæç¤º
    zero_shot_cot_patterns = [
        "ä¸€æ­¥æ­¥æ€è€ƒ", "step by step", "step-by-step", "think step by step",
        "ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—", "ä¸€æ­©ä¸€æ­©"
    ]
    for pattern in zero_shot_cot_patterns:
        if pattern in prompt_lower:
            return "zero_shot_cot"
    
    # æª¢æ¸¬æ€ç¶­éˆæç¤º (ä¸€èˆ¬ CoT)
    cot_patterns = [
        "æ€è€ƒéç¨‹", "æ¨ç†æ­¥é©Ÿ", "é¡¯ç¤ºä½ çš„å·¥ä½œ", "æ€ç¶­éˆ", 
        "show your work", "reasoning process", "chain of thought",
        "æ¨è«–éç¨‹", "æ€è€ƒã®éç¨‹", "æ€è€ƒã®é€£é–"
    ]
    for pattern in cot_patterns:
        if pattern in prompt_lower:
            return "cot"
    
    # æª¢æ¸¬å›é€€æ€è€ƒæç¤º
    step_back_patterns = [
        "å›é€€ä¸€æ­¥", "step back", "å¾Œé€€ä¸€æ­¥", "æ›´å»£æ³›çš„è§’åº¦",
        "broader perspective", "ä¸€æ­©ä¸‹ãŒã£ã¦"
    ]
    for pattern in step_back_patterns:
        if pattern in prompt_lower:
            return "step_back"
    
    # æª¢æ¸¬æ˜¯å¦æœ‰ç¤ºä¾‹ (åˆ¤æ–·æ˜¯é›¶æ¨£æœ¬ã€å–®æ¨£æœ¬é‚„æ˜¯å°‘æ¨£æœ¬)
    # å°‹æ‰¾è¼¸å…¥/è¼¸å‡ºå°çš„æ¨¡å¼
    example_patterns = [
        "ä¾‹å­:", "ç¯„ä¾‹:", "èˆ‰ä¾‹:", "example:", "examples:", "input:", "output:",
        "è¼¸å…¥:", "è¼¸å‡º:", "å…¥åŠ›:", "å‡ºåŠ›:", "ä¾‹:"
    ]
    
    has_examples = False
    example_count = 0
    
    for pattern in example_patterns:
        if pattern in prompt_lower:
            has_examples = True
            example_count += prompt_lower.count(pattern)
    
    if has_examples:
        if example_count == 1:
            return "one_shot"
        elif example_count > 1:
            return "few_shot"
    
    # å¦‚æœæ²’æœ‰æª¢æ¸¬åˆ°ä»»ä½•ç‰¹å®šé¡å‹ï¼Œå‰‡ç‚ºé›¶æ¨£æœ¬æç¤º
    return "zero_shot"

# æ·»åŠ è‡ªå®šç¾© CSS
def add_custom_css():
    st.markdown("""
    <style>
    /* å¢åŠ é¸æ“‡æ¡†å¯¬åº¦ */
    div[data-baseweb="select"] {
        min-width: 200px !important;
    }
    
    /* ç¢ºä¿ä¸‹æ‹‰é¸é …ä¹Ÿè¶³å¤ å¯¬ */
    div[role="listbox"] {
        min-width: 200px !important;
    }
    
    /* å¢åŠ æ•´é«”å…§å®¹å€å¯¬åº¦ */
    .block-container {
        max-width: 1200px;
        padding-top: 1rem;
        padding-bottom: 1rem;
    }
    </style>
    """, unsafe_allow_html=True)

# ä¸»å‡½æ•¸
def main():
    add_custom_css()
    
    # åˆå§‹åŒ–æœƒè©±ç‹€æ…‹
    initialize_session_state()
    
    # èªè¨€é¸æ“‡
    col1, col2, col3 = st.columns([1, 8, 1])
    with col1:
        st.empty()
    with col3:
        selected_language = st.selectbox(
            "Language",
            ["ç¹é«”ä¸­æ–‡", "English", "æ—¥æœ¬èª"],
            index=["zh_TW", "en", "ja"].index(st.session_state.language),
            key="language_selector"
        )
        
        # æ›´æ–°èªè¨€é¸æ“‡
        lang_map = {"ç¹é«”ä¸­æ–‡": "zh_TW", "English": "en", "æ—¥æœ¬èª": "ja"}
        if st.session_state.language != lang_map[selected_language]:
            st.session_state.language = lang_map[selected_language]
            st.rerun()
    
    # é¡¯ç¤ºå´é‚Šæ¬„
    show_sidebar()

    # æ ¹æ“šæ¨¡å¼é¡¯ç¤ºä¸åŒçš„ UI
    if st.session_state.conversation_mode:
        # å°è©±å¼ UI
        render_conversation_ui(t, create_llm)
    else:
        # å‚³çµ±éšæ®µå¼ UI
        show_optimize_ui()

if __name__ == "__main__":
    main()