#!/usr/bin/env python3
"""
æ¸¬è©¦ Gemini æ¨¡å‹æ•´åˆ
"""
import os
import sys
from llm_invoker import LLMFactory

def test_gemini_api():
    """æ¸¬è©¦ Gemini API Key æ¨¡å¼"""
    print("æ¸¬è©¦ Gemini API Key æ¨¡å¼...")
    
    # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        print("âŒ æœªè¨­ç½® GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸")
        return False
    
    try:
        # å‰µå»º Gemini invoker
        gemini = LLMFactory.create_llm("gemini", model="gemini-2.0-flash-exp")
        
        # æ¸¬è©¦é€£æ¥
        is_connected, message = gemini.check_connection()
        print(f"é€£æ¥æ¸¬è©¦: {message}")
        
        if is_connected:
            # æ¸¬è©¦ç°¡å–®å°è©±
            response = gemini.invoke("è«‹ç”¨ä¸€å¥è©±ä»‹ç´¹ä½ è‡ªå·±", temperature=0.7)
            print(f"âœ… Gemini å›æ‡‰: {response['content'][:100]}...")
            print(f"Token ä½¿ç”¨: è¼¸å…¥ {response['usage']['input_tokens']}, è¼¸å‡º {response['usage']['output_tokens']}")
            return True
        else:
            print("âŒ é€£æ¥å¤±æ•—")
            return False
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        return False

def test_vertex_ai():
    """æ¸¬è©¦ Vertex AI æ¨¡å¼"""
    print("\næ¸¬è©¦ Vertex AI æ¨¡å¼...")
    
    # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
    project_id = os.environ.get("GOOGLE_CLOUD_PROJECT")
    if not project_id:
        print("âŒ æœªè¨­ç½® GOOGLE_CLOUD_PROJECT ç’°å¢ƒè®Šæ•¸")
        return False
    
    try:
        # å‰µå»º Vertex AI invoker
        vertex = LLMFactory.create_llm("gemini-vertex", model="gemini-1.5-pro")
        
        # æ¸¬è©¦é€£æ¥
        is_connected, message = vertex.check_connection()
        print(f"é€£æ¥æ¸¬è©¦: {message}")
        
        if is_connected:
            # æ¸¬è©¦ç°¡å–®å°è©±
            response = vertex.invoke("è«‹ç”¨ä¸€å¥è©±ä»‹ç´¹ä½ è‡ªå·±", temperature=0.7)
            print(f"âœ… Vertex AI å›æ‡‰: {response['content'][:100]}...")
            print(f"Token ä½¿ç”¨: è¼¸å…¥ {response['usage']['input_tokens']}, è¼¸å‡º {response['usage']['output_tokens']}")
            return True
        else:
            print("âŒ é€£æ¥å¤±æ•—")
            return False
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        return False

def test_factory():
    """æ¸¬è©¦å·¥å» é¡"""
    print("\næ¸¬è©¦å·¥å» é¡...")
    
    try:
        # ç²å–å¯ç”¨æ¨¡å‹
        models = LLMFactory.get_available_models()
        print("âœ… å¯ç”¨æ¨¡å‹æä¾›è€…:")
        for provider, info in models.items():
            print(f"  - {provider}: {', '.join(info['models'])}")
        
        return True
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        return False

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("é–‹å§‹æ¸¬è©¦ Gemini æ•´åˆ...")
    
    results = []
    
    # æ¸¬è©¦å·¥å» é¡
    results.append(test_factory())
    
    # æ¸¬è©¦ Gemini API
    results.append(test_gemini_api())
    
    # æ¸¬è©¦ Vertex AI
    results.append(test_vertex_ai())
    
    # ç¸½çµ
    print(f"\næ¸¬è©¦çµæœ: {sum(results)}/{len(results)} é€šé")
    
    if all(results):
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦éƒ½é€šéäº†ï¼")
        return 0
    else:
        print("âš ï¸ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç’°å¢ƒè¨­å®š")
        return 1

if __name__ == "__main__":
    sys.exit(main())