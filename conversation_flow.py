#!/usr/bin/env python3
"""
å°è©±æµç¨‹æ§åˆ¶æ¨¡çµ„
ç®¡ç†å°è©±å¼ UI çš„ç‹€æ…‹æ©Ÿå’Œæµç¨‹é‚è¼¯
"""

from typing import Dict, Any, Optional
import logging
import json

from conversation_types import (
    ConversationSession,
    Message,
    MessageRole,
    MessageType,
    ConversationState
)
from prompt_eval import PromptEvaluator
from llm_invoker import ParameterPresets

logger = logging.getLogger(__name__)


class ConversationFlow:
    """å°è©±æµç¨‹æ§åˆ¶å™¨"""

    # å¸¸æ•¸å®šç¾©
    DEFAULT_MAX_TOKENS = 8192  # å°è©±å›æ‡‰çš„é è¨­æœ€å¤§ token æ•¸
    SECTION_DELIMITER = "==="  # Prompt å€æ®µåˆ†éš”ç¬¦

    # éŒ¯èª¤è¨Šæ¯ç¿»è­¯
    ERROR_MESSAGES = {
        "zh_TW": {
            "analysis_error": "æŠ±æ­‰ï¼Œåˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{error}",
            "questions_error": "æŠ±æ­‰ï¼Œç”Ÿæˆæ”¹é€²å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{error}",
            "modification_error": "æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„ä¿®æ”¹è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{error}",
            "conversation_error": "æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{error}"
        },
        "en": {
            "analysis_error": "Sorry, an error occurred during analysis: {error}",
            "questions_error": "Sorry, an error occurred while generating questions: {error}",
            "modification_error": "Sorry, an error occurred while processing your modification request: {error}",
            "conversation_error": "Sorry, an error occurred while processing your question: {error}"
        },
        "ja": {
            "analysis_error": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š{error}",
            "questions_error": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚è³ªå•ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š{error}",
            "modification_error": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ä¿®æ­£ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š{error}",
            "conversation_error": "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã”è³ªå•ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š{error}"
        }
    }

    # LLM Prompt æ¨¡æ¿ç¿»è­¯
    PROMPT_TEMPLATES = {
        "zh_TW": {
            "modification": """åŸºæ–¼ä»¥ä¸‹ç•¶å‰æç¤ºé€²è¡Œèª¿æ•´ï¼š

===ç•¶å‰æç¤º===
{current_prompt}
===

===ç”¨æˆ¶è¦æ±‚===
{user_input}
===

è«‹æ ¹æ“šç”¨æˆ¶è¦æ±‚èª¿æ•´æç¤ºï¼Œä¿æŒå…¶ä»–éƒ¨åˆ†ä¸è®Šã€‚
é‡è¦ï¼šåªè¼¸å‡ºèª¿æ•´å¾Œçš„å®Œæ•´æç¤ºæœ¬èº«ï¼Œä¸è¦åŒ…å«ä»»ä½•æ¨™è¨˜ã€èªªæ˜æˆ–æ ¼å¼ç¬¦è™Ÿã€‚""",
            "conversation": """å°è©±ä¸Šä¸‹æ–‡ï¼š
{context}

===ç•¶å‰å„ªåŒ–çš„æç¤º===
{current_prompt}
===

===ç”¨æˆ¶å•é¡Œ===
{user_input}
===

è«‹æ ¹æ“šå°è©±ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚"""
        },
        "en": {
            "modification": """Adjust based on the current prompt:

===Current Prompt===
{current_prompt}
===

===User Request===
{user_input}
===

Please adjust the prompt according to the user's request, keeping other parts unchanged.
Important: Output ONLY the complete adjusted prompt itself, without any markers, explanations, or formatting symbols.""",
            "conversation": """Conversation context:
{context}

===Current Optimized Prompt===
{current_prompt}
===

===User Question===
{user_input}
===

Please answer the user's question based on the conversation context."""
        },
        "ja": {
            "modification": """ä»¥ä¸‹ã®ç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ã„ã¦èª¿æ•´ã—ã¦ãã ã•ã„ï¼š

===ç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ===
{current_prompt}
===

===ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚===
{user_input}
===

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«å¾“ã£ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª¿æ•´ã—ã€ä»–ã®éƒ¨åˆ†ã¯å¤‰æ›´ã—ãªã„ã§ãã ã•ã„ã€‚
é‡è¦ï¼šèª¿æ•´å¾Œã®å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ¬ä½“ã®ã¿ã‚’å‡ºåŠ›ã—ã€ãƒãƒ¼ã‚«ãƒ¼ã€èª¬æ˜ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¨˜å·ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚""",
            "conversation": """ä¼šè©±ã®æ–‡è„ˆï¼š
{context}

===ç¾åœ¨ã®æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ===
{current_prompt}
===

===ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•===
{user_input}
===

ä¼šè©±ã®æ–‡è„ˆã«åŸºã¥ã„ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚"""
        }
    }

    def __init__(self, session: ConversationSession, llm_instance: Any, language: str = "zh_TW"):
        """
        åˆå§‹åŒ–å°è©±æµç¨‹æ§åˆ¶å™¨

        Args:
            session: å°è©±æœƒè©±
            llm_instance: LLM å¯¦ä¾‹
            language: èªè¨€ä»£ç¢¼
        """
        self.session = session
        self.llm = llm_instance
        self.language = language
        self.evaluator = PromptEvaluator(llm_instance=llm_instance)
        self.state = ConversationState.IDLE

    def _track_token_usage(self, result: Dict[str, Any]):
        """è¿½è¹¤ LLM èª¿ç”¨çš„ token ä½¿ç”¨é‡"""
        usage = result.get("usage", {})
        total_tokens = usage.get("total_tokens", 0)
        if total_tokens > 0:
            self.session.update_token_usage(total_tokens)

    def _get_error_message(self, key: str, error: str) -> str:
        """ç²å–æœ¬åœ°åŒ–çš„éŒ¯èª¤è¨Šæ¯"""
        messages = self.ERROR_MESSAGES.get(self.language, self.ERROR_MESSAGES["zh_TW"])
        return messages.get(key, "Error: {error}").format(error=error)

    def _sanitize_delimiter(self, text: str) -> str:
        """é˜²æ­¢åˆ†éš”ç¬¦æ³¨å…¥ï¼ˆè½‰ç¾©å€æ®µåˆ†éš”ç¬¦ï¼‰"""
        if not text:
            return ""
        # å°‡åˆ†éš”ç¬¦æ›¿æ›ç‚ºå®‰å…¨çš„æ›¿ä»£ç¬¦è™Ÿ
        return text.replace(self.SECTION_DELIMITER, "-" * len(self.SECTION_DELIMITER))

    def _prepare_llm_params(self, preset_name: str) -> Dict[str, Any]:
        """
        æº–å‚™ LLM èª¿ç”¨åƒæ•¸

        Args:
            preset_name: åƒæ•¸é è¨­åç¨±

        Returns:
            æ¸…ç†å¾Œçš„åƒæ•¸å­—å…¸
        """
        preset = ParameterPresets.get_preset(preset_name)
        # ç§»é™¤ descriptionï¼ˆinvoke ä¸æ¥å—æ­¤åƒæ•¸ï¼‰
        llm_params = {k: v for k, v in preset.items() if k != 'description'}
        # åƒ…åœ¨ Preset æœªè¨­å®šæ™‚ä½¿ç”¨é è¨­ max_tokensï¼ˆå®Œå…¨å°Šé‡ Preset è¨­å®šï¼‰
        if 'max_tokens' not in llm_params:
            llm_params['max_tokens'] = self.DEFAULT_MAX_TOKENS
        return llm_params

    def handle_initial_prompt(self, prompt: str) -> Dict[str, Any]:
        """
        è™•ç†åˆå§‹ prompt è¼¸å…¥

        Args:
            prompt: ç”¨æˆ¶è¼¸å…¥çš„æç¤º

        Returns:
            åŒ…å«åˆ†æå’Œå•é¡Œçš„çµæœå­—å…¸
        """
        # è¨˜éŒ„ç”¨æˆ¶è¨Šæ¯
        user_msg = self.session.add_message(
            role=MessageRole.USER,
            msg_type=MessageType.TEXT,
            content=prompt
        )

        # æ›´æ–°æœƒè©±çš„ prompt
        self.session.original_prompt = prompt
        self.session.current_prompt = prompt

        # è‡ªå‹•è§¸ç™¼åˆ†æ
        self.state = ConversationState.ANALYZING
        analysis_result = self.analyze_prompt(prompt)

        # æª¢æŸ¥åˆ†ææ˜¯å¦å¤±æ•—
        if "error" in analysis_result:
            self.state = ConversationState.IDLE
            return {
                "user_message": user_msg,
                "analysis": analysis_result,
                "questions": None,
                "state": self.state
            }

        # è‡ªå‹•ç”Ÿæˆæ”¹é€²å•é¡Œ
        self.state = ConversationState.AWAITING_QUESTIONS
        questions_result = self.generate_questions()

        # æª¢æŸ¥å•é¡Œç”Ÿæˆæ˜¯å¦å¤±æ•—
        if "error" in questions_result:
            return {
                "user_message": user_msg,
                "analysis": analysis_result,
                "questions": questions_result,
                "state": self.state
            }

        return {
            "user_message": user_msg,
            "analysis": analysis_result,
            "questions": questions_result,
            "state": self.state
        }

    def analyze_prompt(self, prompt: str) -> Dict[str, Any]:
        """
        åŸ·è¡Œ prompt åˆ†æ

        Args:
            prompt: è¦åˆ†æçš„æç¤º

        Returns:
            åˆ†æçµæœå­—å…¸
        """
        try:
            # èª¿ç”¨ PromptEvaluator é€²è¡Œåˆ†æ
            analysis = self.evaluator.analyze_prompt(prompt, self.language)

            # æ ¼å¼åŒ–åˆ†æå…§å®¹
            analysis_content = self._format_analysis_content(analysis)

            # æ·»åŠ åˆ†æçµæœè¨Šæ¯
            analysis_msg = self.session.add_message(
                role=MessageRole.ASSISTANT,
                msg_type=MessageType.ANALYSIS,
                content=analysis_content,
                analysis_data=analysis
            )

            # ä¿å­˜åˆ†æçµæœ
            self.session.last_analysis = analysis

            return {
                "message": analysis_msg,
                "analysis": analysis
            }

        except Exception as e:
            # éŒ¯èª¤è™•ç†ï¼šè¿”å›å‹å¥½çš„éŒ¯èª¤è¨Šæ¯
            logger.error("Error analyzing prompt", exc_info=True)
            error_msg = self._get_error_message("analysis_error", str(e))
            error_message = self.session.add_message(
                role=MessageRole.ASSISTANT,
                msg_type=MessageType.TEXT,
                content=error_msg
            )
            return {
                "message": error_message,
                "error": str(e)
            }

    def generate_questions(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ”¹é€²å•é¡Œ

        Returns:
            å•é¡Œçµæœå­—å…¸
        """
        if not self.session.last_analysis:
            raise ValueError("å¿…é ˆå…ˆåŸ·è¡Œåˆ†ææ‰èƒ½ç”Ÿæˆå•é¡Œ")

        try:
            # èª¿ç”¨ PromptEvaluator ç”Ÿæˆå•é¡Œ
            questions = self.evaluator.generate_questions(
                self.session.last_analysis,
                self.language
            )

            # æ ¼å¼åŒ–å•é¡Œå…§å®¹
            questions_content = self._format_questions_content(questions)

            # æ·»åŠ å•é¡Œè¨Šæ¯
            questions_msg = self.session.add_message(
                role=MessageRole.ASSISTANT,
                msg_type=MessageType.QUESTIONS,
                content=questions_content,
                questions_data=questions
            )

            # ä¿å­˜å¾…å›ç­”çš„å•é¡Œ
            self.session.pending_questions = questions

            return {
                "message": questions_msg,
                "questions": questions
            }

        except Exception as e:
            # éŒ¯èª¤è™•ç†ï¼šè¿”å›å‹å¥½çš„éŒ¯èª¤è¨Šæ¯
            logger.error("Error generating questions", exc_info=True)
            error_msg = self._get_error_message("questions_error", str(e))
            error_message = self.session.add_message(
                role=MessageRole.ASSISTANT,
                msg_type=MessageType.TEXT,
                content=error_msg
            )
            return {
                "message": error_message,
                "error": str(e)
            }

    def handle_questions_response(self, responses: Dict[str, Any]) -> Dict[str, Any]:
        """
        è™•ç†ç”¨æˆ¶å°æ”¹é€²å•é¡Œçš„å›ç­”

        Args:
            responses: ç”¨æˆ¶å›ç­”å­—å…¸

        Returns:
            å„ªåŒ–çµæœå­—å…¸
        """
        # è¨˜éŒ„ç”¨æˆ¶å›ç­”
        responses_content = self._format_responses_content(responses)
        user_msg = self.session.add_message(
            role=MessageRole.USER,
            msg_type=MessageType.TEXT,
            content=responses_content,
            metadata={"responses": responses}
        )

        # ä¿å­˜å›ç­”
        self.session.question_answers = responses

        # åŸ·è¡Œå„ªåŒ–
        self.state = ConversationState.OPTIMIZING
        optimization_result = self.optimize_prompt(responses)

        self.state = ConversationState.COMPLETED

        return {
            "user_message": user_msg,
            "optimization": optimization_result,
            "state": self.state
        }

    def optimize_prompt(self, responses: Dict[str, Any]) -> Dict[str, Any]:
        """
        åŸ·è¡Œ prompt å„ªåŒ–

        Args:
            responses: ç”¨æˆ¶å›ç­”å­—å…¸

        Returns:
            å„ªåŒ–çµæœå­—å…¸
        """
        if not self.session.last_analysis:
            raise ValueError("å¿…é ˆå…ˆåŸ·è¡Œåˆ†ææ‰èƒ½å„ªåŒ–")

        try:
            # èª¿ç”¨ PromptEvaluator é€²è¡Œå„ªåŒ–
            result = self.evaluator.optimize_prompt(
                self.session.current_prompt,
                responses,
                self.session.last_analysis,
                self.language
            )

            # æ·»åŠ å„ªåŒ–çµæœè¨Šæ¯
            optimization_msg = self.session.add_message(
                role=MessageRole.ASSISTANT,
                msg_type=MessageType.OPTIMIZATION,
                content=result["enhanced_prompt"],
                optimization_data=result
            )

            # æ›´æ–°æœƒè©±ç‹€æ…‹
            self.session.last_optimization = result
            self.session.current_prompt = result["enhanced_prompt"]
            self.session.iteration_count += 1

            return {
                "message": optimization_msg,
                "result": result
            }

        except Exception as e:
            # éŒ¯èª¤è™•ç†ï¼šè¿”å›å‹å¥½çš„éŒ¯èª¤è¨Šæ¯ï¼ˆä½¿ç”¨ conversation_error ä½œç‚ºé€šç”¨éŒ¯èª¤ï¼‰
            logger.error("Error optimizing prompt", exc_info=True)
            error_msg = self._get_error_message("conversation_error", str(e))
            error_message = self.session.add_message(
                role=MessageRole.ASSISTANT,
                msg_type=MessageType.TEXT,
                content=error_msg
            )
            return {
                "message": error_message,
                "error": str(e)
            }

    def handle_followup_message(self, user_input: str) -> Dict[str, Any]:
        """
        è™•ç†å„ªåŒ–å®Œæˆå¾Œçš„æŒçºŒå°è©±ï¼ˆå°‡èª¿æ•´è¦æ±‚è¦–ç‚ºæ–°çš„å„ªåŒ–éœ€æ±‚ï¼‰

        Args:
            user_input: ç”¨æˆ¶è¼¸å…¥

        Returns:
            è™•ç†çµæœå­—å…¸
        """
        # è¨˜éŒ„ç”¨æˆ¶è¨Šæ¯
        user_msg = self.session.add_message(
            role=MessageRole.USER,
            msg_type=MessageType.TEXT,
            content=user_input
        )

        # æ ¹æ“šç”¨æˆ¶æ„åœ–æ±ºå®šä¸‹ä¸€æ­¥
        intent = self._classify_user_intent(user_input)

        if intent == "iterate":
            # ç”¨æˆ¶æƒ³è¦å†æ¬¡å„ªåŒ–
            return self.handle_initial_prompt(self.session.current_prompt)
        elif intent == "modify":
            # ç”¨æˆ¶æƒ³è¦èª¿æ•´ï¼šè½‰æ›ç‚ºå®Œæ•´çš„é‡æ–°å„ªåŒ–æµç¨‹
            return self._handle_adjustment_as_optimization(user_input)
        else:
            # ä¸€èˆ¬æ€§å•ç­”å°è©±
            return self._handle_general_conversation(user_input)

    def _classify_user_intent(self, user_input: str) -> str:
        """
        åˆ†é¡ç”¨æˆ¶æ„åœ–ï¼ˆä½¿ç”¨é—œéµå­—åŒ¹é…ï¼‰

        Args:
            user_input: ç”¨æˆ¶è¼¸å…¥

        Returns:
            æ„åœ–é¡å‹ï¼šiterate, modify, general
        """
        input_lower = user_input.lower()

        # è¿­ä»£é—œéµå­—ï¼ˆå„ªå…ˆæª¢æŸ¥ï¼Œè¼ƒç‚ºæ˜ç¢ºï¼‰
        iterate_keywords = ["å†æ¬¡å„ªåŒ–", "ç¹¼çºŒå„ªåŒ–", "optimize again", "iterate", "ã‚‚ã†ä¸€åº¦æœ€é©åŒ–"]
        if any(kw in input_lower for kw in iterate_keywords):
            return "iterate"

        # ä¿®æ”¹é—œéµå­—
        modify_keywords = ["ä¿®æ”¹", "èª¿æ•´", "æ”¹ä¸€ä¸‹", "modify", "adjust", "change", "å¤‰æ›´"]
        if any(kw in input_lower for kw in modify_keywords):
            return "modify"

        return "general"

    def _handle_adjustment_as_optimization(self, user_input: str) -> Dict[str, Any]:
        """
        å°‡ç”¨æˆ¶çš„èª¿æ•´è¦æ±‚è½‰æ›ç‚ºå®Œæ•´çš„å„ªåŒ–æµç¨‹

        Args:
            user_input: ç”¨æˆ¶çš„èª¿æ•´è¦æ±‚ï¼ˆå¦‚ã€Œæ›´æ­£å¼ä¸€é»ã€ã€ã€ŒåŠ ä¸Šç¯„ä¾‹ã€ï¼‰

        Returns:
            å„ªåŒ–çµæœå­—å…¸
        """
        # ä½¿ç”¨ LLM å°‡è‡ªç”±æ–‡å­—è¦æ±‚è½‰æ›ç‚ºçµæ§‹åŒ–çš„å„ªåŒ–åƒæ•¸ï¼ˆå¢é‡ï¼‰
        delta_responses = self._convert_request_to_responses(user_input)

        # åˆä½µä¹‹å‰çš„åƒæ•¸å’Œæ–°çš„èª¿æ•´ï¼ˆé—œéµï¼šä¿ç•™æ­·å²è¨­å®šï¼‰
        combined_responses = self.session.question_answers.copy() if self.session.question_answers else {}
        combined_responses.update(delta_responses)

        # ä½¿ç”¨ç•¶å‰ prompt å’Œåˆä½µå¾Œçš„åƒæ•¸é‡æ–°å„ªåŒ–
        if not self.session.last_analysis:
            # å¦‚æœæ²’æœ‰åˆ†æçµæœï¼Œå…ˆé‡æ–°åˆ†æ
            analysis_result = self.analyze_prompt(self.session.current_prompt)
            if "error" in analysis_result:
                return analysis_result

        # åŸ·è¡Œå„ªåŒ–
        optimization_result = self.optimize_prompt(combined_responses)

        # ä¿å­˜åˆä½µå¾Œçš„åƒæ•¸ä¾›ä¸‹æ¬¡ä½¿ç”¨
        self.session.question_answers = combined_responses

        return optimization_result

    def _convert_request_to_responses(self, user_request: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM å°‡è‡ªç”±æ–‡å­—èª¿æ•´è¦æ±‚è½‰æ›ç‚ºçµæ§‹åŒ–åƒæ•¸

        Args:
            user_request: ç”¨æˆ¶è¦æ±‚ï¼ˆå¦‚ã€Œæ›´æ­£å¼ä¸€é»ã€ã€ã€ŒåŠ ä¸Šç¯„ä¾‹ã€ï¼‰

        Returns:
            user_responses å­—å…¸
        """
        # é˜²æ­¢åˆ†éš”ç¬¦æ³¨å…¥
        sanitized_request = self._sanitize_delimiter(user_request)

        # ä½¿ç”¨è‹±æ–‡ promptï¼Œä½†è¦æ±‚è¼¸å‡ºç¹é«”ä¸­æ–‡åƒæ•¸å€¼
        # åŸå› ï¼šPromptEvaluator åº•å±¤çš„ prompts.yaml ä½¿ç”¨ä¸­æ–‡åƒæ•¸å€¼ï¼Œä¿æŒä¸€è‡´æ€§
        conversion_prompt = f"""Analyze the following user's prompt adjustment request (may be in any language) and convert it to structured parameters.

User Request: {sanitized_request}

Respond in JSON format with these optional fields (only include relevant ones):
{{
  "role": "è§’è‰²å®šç¾©ï¼ˆç”¨ç¹é«”ä¸­æ–‡ï¼‰",
  "format": "è¼¸å‡ºæ ¼å¼èªªæ˜ï¼ˆç”¨ç¹é«”ä¸­æ–‡ï¼Œå¦‚ï¼šåŒ…å«å…·é«”ç¯„ä¾‹ã€JSONæ ¼å¼ç­‰ï¼‰",
  "detail": "è©³ç´°ç¨‹åº¦æˆ–èªæ°£ï¼ˆç”¨ç¹é«”ä¸­æ–‡ï¼Œå¦‚ï¼šç°¡æ½”ã€æ­£å¼ä¸”å°ˆæ¥­ã€è©³ç´°ç­‰ï¼‰",
  "scope": "å›ç­”ç¯„åœï¼ˆç”¨ç¹é«”ä¸­æ–‡ï¼Œå¦‚ï¼šèšç„¦ã€å¯¬æ³›ï¼‰",
  "reasoning": true/false
}}

Examples:
- "make it more formal" â†’ {{"detail": "æ­£å¼ä¸”å°ˆæ¥­"}}
- "æ›´æ­£å¼ä¸€é»" â†’ {{"detail": "æ­£å¼ä¸”å°ˆæ¥­"}}
- "add examples" â†’ {{"format": "åŒ…å«å…·é«”ç¯„ä¾‹"}}
- "åŠ ä¸Šç¯„ä¾‹" â†’ {{"format": "åŒ…å«å…·é«”ç¯„ä¾‹"}}
- "keep it brief" â†’ {{"detail": "ç°¡æ½”"}}
- "ç°¡çŸ­ä¸€äº›" â†’ {{"detail": "ç°¡æ½”"}}

Important: Output values in Traditional Chinese. Output ONLY the JSON, no other text."""

        result = None  # åˆå§‹åŒ–é¿å… UnboundLocalError
        try:
            llm_params = self._prepare_llm_params("ç²¾ç¢º")
            llm_params['max_tokens'] = 500  # çŸ­å›ç­”å³å¯
            result = self.llm.invoke(
                prompt=conversion_prompt,
                **llm_params
            )

            # è¿½è¹¤ token ä½¿ç”¨é‡
            self._track_token_usage(result)

            # è§£æ JSONï¼ˆå„ªå…ˆè™•ç† Markdown code blocksï¼‰
            response_text = result["content"].strip()

            # è™•ç† Markdown code block
            if "```json" in response_text:
                json_str = response_text.split("```json")[1].split("```")[0].strip()
            elif "```" in response_text:
                json_str = response_text.split("```")[1].split("```")[0].strip()
            else:
                # ä½¿ç”¨ find/rfind æå– JSONï¼ˆè™•ç†å·¢ç‹€ç‰©ä»¶ï¼‰
                start_idx = response_text.find('{')
                end_idx = response_text.rfind('}')
                if start_idx != -1 and end_idx != -1:
                    json_str = response_text[start_idx : end_idx + 1]
                else:
                    json_str = response_text  # å˜—è©¦ç›´æ¥è§£æ

            user_responses = json.loads(json_str)

            # éæ¿¾ç©ºå€¼å’Œ null
            return {k: v for k, v in user_responses.items() if v is not None and v != ""}

        except Exception as e:
            # å®‰å…¨è¨ªå• result
            raw_content = result.get('content', 'N/A')[:500] if result and isinstance(result, dict) else 'N/A'
            logger.error(f"Error converting request to responses: {e}. Raw response: {raw_content}")
            # å›é€€ï¼šä½¿ç”¨ç°¡å–®çš„é—œéµå­—åŒ¹é…
            return self._fallback_request_conversion(user_request)

    def _fallback_request_conversion(self, user_request: str) -> Dict[str, Any]:
        """å›é€€çš„ç°¡å–®è½‰æ›é‚è¼¯"""
        responses = {}
        user_lower = user_request.lower()

        # ç°¡å–®é—œéµå­—åŒ¹é…
        if any(kw in user_lower for kw in ["æ­£å¼", "formal", "professional", "ãƒ•ã‚©ãƒ¼ãƒãƒ«"]):
            responses["detail"] = "æ­£å¼ä¸”å°ˆæ¥­"
        if any(kw in user_lower for kw in ["ç¯„ä¾‹", "example", "ä¾‹", "ã‚µãƒ³ãƒ—ãƒ«"]):
            responses["format"] = "åŒ…å«å…·é«”ç¯„ä¾‹"
        if any(kw in user_lower for kw in ["ç°¡çŸ­", "brief", "concise", "çŸ­", "ç°¡æ½”"]):
            responses["detail"] = "ç°¡æ½”"

        return responses

    def _handle_general_conversation(self, user_input: str) -> Dict[str, Any]:
        """
        è™•ç†ä¸€èˆ¬æ€§å°è©±

        Args:
            user_input: ç”¨æˆ¶è¼¸å…¥

        Returns:
            è™•ç†çµæœå­—å…¸
        """
        # æ§‹å»ºå°è©±ä¸Šä¸‹æ–‡
        context = self._build_conversation_context()

        # ä½¿ç”¨æœ¬åœ°åŒ–çš„æç¤ºæ¨¡æ¿ï¼Œä¸¦é˜²æ­¢åˆ†éš”ç¬¦æ³¨å…¥
        templates = self.PROMPT_TEMPLATES.get(self.language, self.PROMPT_TEMPLATES["zh_TW"])
        conversation_prompt = templates["conversation"].format(
            context=context,
            current_prompt=self._sanitize_delimiter(self.session.current_prompt),
            user_input=self._sanitize_delimiter(user_input)
        )

        try:
            # ä½¿ç”¨å¹³è¡¡åƒæ•¸é è¨­ï¼ˆé©åˆä¸€èˆ¬å°è©±ï¼‰
            llm_params = self._prepare_llm_params("å¹³è¡¡")
            result = self.llm.invoke(
                prompt=conversation_prompt,
                **llm_params
            )

            # è¿½è¹¤ token ä½¿ç”¨é‡
            self._track_token_usage(result)

            response_content = result["content"].strip()

            # æ·»åŠ  AI å›æ‡‰
            response_msg = self.session.add_message(
                role=MessageRole.ASSISTANT,
                msg_type=MessageType.TEXT,
                content=response_content
            )

            return {
                "message": response_msg,
                "response": response_content
            }

        except Exception as e:
            # éŒ¯èª¤è™•ç†ï¼šè¿”å›å‹å¥½çš„éŒ¯èª¤è¨Šæ¯
            logger.error("Error handling general conversation", exc_info=True)
            error_msg = self._get_error_message("conversation_error", str(e))
            response_msg = self.session.add_message(
                role=MessageRole.ASSISTANT,
                msg_type=MessageType.TEXT,
                content=error_msg
            )
            return {
                "message": response_msg,
                "error": str(e)
            }

    def _build_conversation_context(self) -> str:
        """
        æ§‹å»ºå°è©±ä¸Šä¸‹æ–‡ï¼ˆæœ€è¿‘çš„è¨Šæ¯ï¼‰

        Returns:
            æ ¼å¼åŒ–çš„å°è©±ä¸Šä¸‹æ–‡
        """
        # ç²å–æœ€è¿‘ 5 æ¢è¨Šæ¯
        recent_messages = self.session.messages[-5:] if len(self.session.messages) > 5 else self.session.messages

        context_lines = []
        for msg in recent_messages:
            role_label = "ç”¨æˆ¶" if msg.role == MessageRole.USER else "AI åŠ©æ‰‹"
            # ä¸æˆªæ–·å…§å®¹ - Prompt Engineering å·¥å…·éœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡
            # Token ç®¡ç†ç”± LLM invoke å±¤è™•ç†
            context_lines.append(f"{role_label}: {msg.content}")

        return "\n".join(context_lines)

    def _format_analysis_content(self, analysis: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–åˆ†æçµæœå…§å®¹

        Args:
            analysis: åˆ†æçµæœå­—å…¸

        Returns:
            æ ¼å¼åŒ–çš„æ–‡å­—å…§å®¹
        """
        content_parts = [
            "ğŸ“Š æç¤ºåˆ†æçµæœ",
            "",
            f"å®Œæ•´æ€§ï¼š{analysis.get('completeness_score', 0)}/10",
            f"æ¸…æ™°åº¦ï¼š{analysis.get('clarity_score', 0)}/10",
            f"çµæ§‹æ€§ï¼š{analysis.get('structure_score', 0)}/10",
            f"å…·é«”æ€§ï¼š{analysis.get('specificity_score', 0)}/10",
            "",
            f"æç¤ºé¡å‹ï¼š{analysis.get('prompt_type', 'æœªçŸ¥')}",
            f"è¤‡é›œåº¦ï¼š{analysis.get('complexity_level', 'æœªçŸ¥')}"
        ]

        return "\n".join(content_parts)

    def _format_questions_content(self, questions: list) -> str:
        """
        æ ¼å¼åŒ–å•é¡Œå…§å®¹

        Args:
            questions: å•é¡Œåˆ—è¡¨

        Returns:
            æ ¼å¼åŒ–çš„æ–‡å­—å…§å®¹
        """
        if not questions:
            return "æ²’æœ‰éœ€è¦å›ç­”çš„å•é¡Œã€‚"

        content_parts = ["ğŸ’¡ è®“æˆ‘å€‘ä¸€èµ·æ”¹é€²æ‚¨çš„æç¤º", ""]
        for i, q in enumerate(questions, 1):
            content_parts.append(f"{i}. {q.get('question', '')}")

        return "\n".join(content_parts)

    def _format_responses_content(self, responses: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–ç”¨æˆ¶å›ç­”å…§å®¹

        Args:
            responses: å›ç­”å­—å…¸

        Returns:
            æ ¼å¼åŒ–çš„æ–‡å­—å…§å®¹
        """
        content_parts = ["æˆ‘çš„å›ç­”ï¼š", ""]
        for key, value in responses.items():
            content_parts.append(f"- {key}: {value}")

        return "\n".join(content_parts)

    def reset_conversation(self):
        """é‡ç½®å°è©±ç‹€æ…‹"""
        self.session.clear_messages()
        self.session.current_prompt = ""
        self.session.original_prompt = ""
        self.session.last_analysis = None
        self.session.last_optimization = None
        self.session.pending_questions = None
        self.session.question_answers = {}
        self.session.iteration_count = 0
        self.state = ConversationState.IDLE

    def can_optimize(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦å¯ä»¥åŸ·è¡Œå„ªåŒ–"""
        return (
            self.session.last_analysis is not None and
            self.session.question_answers is not None and
            len(self.session.question_answers) > 0
        )

    def get_state_summary(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰ç‹€æ…‹æ‘˜è¦"""
        return {
            "state": self.state.value,
            "message_count": len(self.session.messages),
            "iteration_count": self.session.iteration_count,
            "has_analysis": self.session.last_analysis is not None,
            "has_optimization": self.session.last_optimization is not None,
            "pending_questions_count": len(self.session.pending_questions) if self.session.pending_questions else 0
        }
